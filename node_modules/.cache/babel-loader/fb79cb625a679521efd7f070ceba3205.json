{"ast":null,"code":"function n(a) {\n  return a.name.value;\n}\n\nfunction t(a) {\n  return a.selectionSet ? a.selectionSet.selections : [];\n}\n\nfunction aa(a, b) {\n  var e,\n      f,\n      g,\n      h,\n      c = {},\n      d = 0;\n\n  if (a.arguments && a.arguments.length) {\n    for (e = 0, f = a.arguments.length; e < f; e++) {\n      null != (h = valueFromASTUntyped((g = a.arguments[e]).value, b)) && (c[n(g)] = h, d++);\n    }\n  }\n\n  return 0 < d ? c : null;\n}\n\nfunction ba(a, b) {\n  var d,\n      e,\n      f,\n      g,\n      h,\n      c = {};\n\n  if (!b) {\n    return c;\n  }\n\n  if (a.variableDefinitions) {\n    for (d = 0, e = a.variableDefinitions.length; d < e; d++) {\n      c[g = n((f = a.variableDefinitions[d]).variable)] = void 0 === b[g] && f.defaultValue ? valueFromASTUntyped(f.defaultValue, b) : b[g];\n    }\n  }\n\n  for (h in b) {\n    h in c || (c[h] = b[h]);\n  }\n\n  return c;\n}\n\nfunction x(a, b) {\n  var c = \"\";\n  b.kind === Kind.INLINE_FRAGMENT ? c = a ? 'Inline Fragment on \"' + a + '\"' : \"Inline Fragment\" : b.kind === Kind.OPERATION_DEFINITION ? c = (b.name ? '\"' + b.name.value + '\"' : \"Unnamed\") + \" \" + b.operation : b.kind === Kind.FRAGMENT_DEFINITION && (c = '\"' + b.name.value + '\" Fragment');\n  c && u.push(c);\n}\n\nfunction da() {\n  return u.length ? \"\\n(Caused At: \" + u.join(\", \") + \")\" : \"\";\n}\n\nfunction y(a, b, c) {\n  if (!a) {\n    throw a = b || \"Minfied Error #\" + c + \"\\n\", \"production\" !== process.env.NODE_ENV && (a += da()), (c = Error(a + \"\\nhttps://bit.ly/2XbVrpR#\" + c)).name = \"Graphcache Error\", c;\n  }\n}\n\nfunction A(a, b) {\n  ca.has(a) || (console.warn(a + da() + \"\\nhttps://bit.ly/2XbVrpR#\" + b), ca.add(a));\n}\n\nfunction C(a) {\n  for (var b = 0; b < a.definitions.length; b++) {\n    if (a.definitions[b].kind === Kind.OPERATION_DEFINITION) {\n      return a.definitions[b];\n    }\n  }\n\n  y(!1, \"production\" !== process.env.NODE_ENV ? \"Invalid GraphQL document: All GraphQL documents must contain an OperationDefinitionnode for a query, subscription, or mutation.\" : \"\", 1);\n}\n\nfunction D(a) {\n  var b, c, d;\n\n  for (b = {}, c = 0; c < a.definitions.length; c++) {\n    (d = a.definitions[c]).kind === Kind.FRAGMENT_DEFINITION && (b[n(d)] = d);\n  }\n\n  return b;\n}\n\nfunction ea(a, b) {\n  var d,\n      e,\n      f,\n      c = a.directives;\n\n  if (!c) {\n    return !0;\n  }\n\n  for (d = 0, e = c.length; d < e; d++) {\n    if ((\"include\" === (a = n(f = c[d])) || \"skip\" === a) && f.arguments && f.arguments[0] && \"if\" === n(f.arguments[0])) {\n      return b = valueFromASTUntyped(f.arguments[0].value, b), \"include\" === a ? !!b : !b;\n    }\n  }\n\n  return !0;\n}\n\nfunction fa(a) {\n  for (var b = {}, c = 0; c < a.length; c++) {\n    b[a[c].name] = a[c];\n  }\n\n  return b;\n}\n\nfunction na(a, b, c) {\n  if (ia.test(c)) {\n    return !0;\n  }\n\n  return !!(a = F(a, b, c)) && \"NON_NULL\" !== a.type.kind;\n}\n\nfunction oa(a, b, c) {\n  if (!(a = F(a, b, c))) {\n    return !1;\n  }\n\n  return \"LIST\" === (a = \"NON_NULL\" === a.type.kind ? a.type.ofType : a.type).kind && \"NON_NULL\" !== a.ofType.kind;\n}\n\nfunction F(a, b, c) {\n  pa(a, b);\n  a = a.types[b].fields[c];\n  \"production\" !== process.env.NODE_ENV && (a || A(\"Invalid field: The field `\" + c + \"` does not exist on `\" + b + \"`, but the GraphQL document expects it to exist.\\nTraversal will continue, however this may lead to undefined behavior!\", 4));\n  return a;\n}\n\nfunction pa(a, b) {\n  y(a.types[b] && \"OBJECT\" === a.types[b].kind, \"production\" !== process.env.NODE_ENV ? \"Invalid Object type: The type `\" + b + \"` is not an object in the defined schema, but the GraphQL document is traversing it.\" : \"\", 3);\n}\n\nfunction qa(a) {\n  \"production\" !== process.env.NODE_ENV && A(\"Invalid resolver: `\" + a + \"` is not in the defined schema, but the `resolvers` option is referencing it.\", 23);\n}\n\nfunction I(a, b) {\n  return b ? a + \"(\" + stringifyVariables(b) + \")\" : a;\n}\n\nfunction ra(a) {\n  var b = a.indexOf(\"(\");\n  return -1 < b ? {\n    fieldKey: a,\n    fieldName: a.slice(0, b),\n    arguments: JSON.parse(a.slice(b + 1, -1))\n  } : {\n    fieldKey: a,\n    fieldName: a,\n    arguments: null\n  };\n}\n\nfunction sa(a, b) {\n  return a.replace(/\\./g, \"%2e\") + \".\" + b;\n}\n\nfunction ta(a) {\n  var b = a.indexOf(\".\");\n  return {\n    entityKey: a.slice(0, b).replace(/%2e/g, \".\"),\n    fieldKey: a = a.slice(b + 1)\n  };\n}\n\nfunction J() {\n  return Object.create(null);\n}\n\nfunction S(a, b, c, d) {\n  K = a;\n  L = b;\n  M = J();\n  Q = !!d;\n  \"production\" !== process.env.NODE_ENV && (u.length = 0);\n  c ? d || 0 < b.optimisticOrder.length ? (d || b.commutativeKeys.has(c) ? d && b.commutativeKeys.delete(c) : ua(b, c), P = c, -1 === b.optimisticOrder.indexOf(c) && b.optimisticOrder.unshift(c), b.refLock[c] || (b.refLock[c] = J(), b.links.optimistic[c] = new Map(), b.records.optimistic[c] = new Map())) : (P = null, va(b, c)) : P = null;\n}\n\nfunction T() {\n  \"production\" !== process.env.NODE_ENV && ya();\n  var b = L,\n      c = P;\n  Q = !1;\n  P = null;\n\n  if (c && -1 < b.optimisticOrder.indexOf(c)) {\n    for (c = b.optimisticOrder.length; 0 <= --c && b.refLock[b.optimisticOrder[c]] && b.commutativeKeys.has(b.optimisticOrder[c]);) {\n      Aa(b.optimisticOrder[c]);\n    }\n  }\n\n  M = L = K = null;\n  \"production\" !== process.env.NODE_ENV && (u.length = 0);\n  \"test\" === process.env.NODE_ENV || b.defer || (b.defer = !0, Promise.resolve().then(function a() {\n    S(\"read\", b, null);\n    L.gc.forEach(wa);\n    !function xa() {\n      function a(a) {\n        var f,\n            c = ta(a),\n            e = c.entityKey;\n        void 0 !== (f = W(e, c = c.fieldKey)) ? b[a] = \":\" + stringifyVariables(f) : void 0 !== (f = V(e, c)) ? b[a] = stringifyVariables(f) : b[a] = void 0;\n      }\n\n      if (L.storage) {\n        Q = !0;\n        K = \"read\";\n        var b = J();\n        L.persist.forEach(a);\n        Q = !1;\n        L.storage.writeData(b);\n        L.persist.clear();\n      }\n    }();\n    T();\n    b.defer = !1;\n  }));\n}\n\nfunction ya() {\n  y(null !== M, \"production\" !== process.env.NODE_ENV ? \"Invalid Cache call: The cache may only be accessed or mutated duringoperations like write or query, or as part of its resolvers, updaters, or optimistic configs.\" : \"\", 2);\n  return M;\n}\n\nfunction Ba(a, b, c, d) {\n  var e = (a = P ? a.optimistic[P] : a.base).get(b);\n  void 0 === e && a.set(b, e = J());\n  void 0 !== d || P ? e[c] = d : delete e[c];\n}\n\nfunction Ca(a, b, c) {\n  var d, e, f, g, h, k;\n\n  for (e = !Q && \"read\" === K && P && L.commutativeKeys.has(P), f = 0, g = L.optimisticOrder.length; f < g; f++) {\n    h = L.optimisticOrder[f];\n    e = e && h !== P;\n\n    if (!(!(k = a.optimistic[h]) || e && L.commutativeKeys.has(h) || Q && \"write\" !== K && !L.commutativeKeys.has(h)) && void 0 !== (d = k.get(b)) && c in d) {\n      return d[c];\n    }\n  }\n\n  return void 0 !== (d = a.base.get(b)) ? d[c] : void 0;\n}\n\nfunction Da(a, b, c, d) {\n  var e = void 0 !== b[c] ? b[c] : 0;\n  b = b[c] = e + d | 0;\n  void 0 !== a && (0 >= b ? a.add(c) : 0 >= e && 0 < b && a.delete(c));\n}\n\nfunction Ea(a, b, c, d) {\n  var e, f, g;\n\n  if (\"string\" == typeof c) {\n    Da(a, b, c, d);\n  } else if (Array.isArray(c)) {\n    for (e = 0, f = c.length; e < f; e++) {\n      (g = c[e]) && Da(a, b, g, d);\n    }\n  }\n}\n\nfunction Fa(a, b, c) {\n  if (void 0 !== c) {\n    for (var d in c) {\n      b.has(d) || (a.push(ra(d)), b.add(d));\n    }\n  }\n}\n\nfunction Ga(a, b, c, d) {\n  var e, f, g;\n  Fa(a, b, d.base.get(c));\n\n  for (e = 0, f = L.optimisticOrder.length; e < f; e++) {\n    void 0 !== (g = d.optimistic[L.optimisticOrder[e]]) && Fa(a, b, g.get(c));\n  }\n}\n\nfunction wa(a, b, c) {\n  var d, e;\n\n  if (0 < (L.refCount[a] || 0)) {\n    c.delete(a);\n  } else {\n    for (d in L.refLock) {\n      if (0 < ((b = L.refLock[d])[a] || 0)) {\n        return;\n      }\n\n      delete b[a];\n    }\n\n    delete L.refCount[a];\n    c.delete(a);\n    L.records.base.delete(a);\n\n    if (d = L.links.base.get(a)) {\n      L.links.base.delete(a);\n\n      for (e in d) {\n        Ea(c, L.refCount, d[e], -1);\n      }\n    }\n  }\n}\n\nfunction U(a, b) {\n  \"__typename\" !== b && (a !== L.queryRootKey ? M[a] = !0 : void 0 !== b && (M[a + \".\" + b] = !0));\n}\n\nfunction V(a, b) {\n  U(a, b);\n  return Ca(L.records, a, b);\n}\n\nfunction W(a, b) {\n  U(a, b);\n  return Ca(L.links, a, b);\n}\n\nfunction X(a, b, c) {\n  U(a, b);\n  !Q && L.storage && L.persist.add(sa(a, b));\n  Ba(L.records, a, b, c);\n}\n\nfunction Ha(a, b, c) {\n  var e,\n      f,\n      g,\n      d = L;\n\n  if (P) {\n    e = d.refLock[P] || (d.refLock[P] = J());\n    f = d.links.optimistic[P];\n  } else {\n    e = d.refCount;\n    f = d.links.base;\n    g = d.gc;\n  }\n\n  f = (f = f && f.get(a)) && f[b];\n  U(a, b);\n  !Q && L.storage && L.persist.add(sa(a, b));\n  Ba(d.links, a, b, c);\n  Ea(g, e, f, -1);\n  Ea(g, e, c, 1);\n}\n\nfunction ua(a, b) {\n  var c = a.optimisticOrder.indexOf(b);\n  -1 === c ? a.optimisticOrder.unshift(b) : a.commutativeKeys.has(b) || (Ia(a, b), a.optimisticOrder.splice(c, 1), a.optimisticOrder.unshift(b));\n  a.commutativeKeys.add(b);\n}\n\nfunction Ia(a, b) {\n  a.refLock[b] && (delete a.refLock[b], delete a.records.optimistic[b], delete a.links.optimistic[b]);\n}\n\nfunction va(a, b) {\n  var c = a.optimisticOrder.indexOf(b);\n  -1 < c && (a.optimisticOrder.splice(c, 1), a.commutativeKeys.delete(b));\n  Ia(a, b);\n}\n\nfunction Ja(a, b) {\n  for (var c in a) {\n    Ha(b, c, a[c]);\n  }\n}\n\nfunction Ka(a, b) {\n  for (var c in a) {\n    X(b, c, a[c]);\n  }\n}\n\nfunction Aa(a) {\n  var c,\n      b = M;\n  M = J();\n  (c = L.links.optimistic[a]) && c.forEach(Ja);\n  (c = L.records.optimistic[a]) && c.forEach(Ka);\n  M = b;\n  va(L, a);\n}\n\nfunction La(a) {\n  var b = L.links,\n      c = L.records,\n      d = [],\n      e = new Set();\n  U(a);\n  Ga(d, e, a, b);\n  Ga(d, e, a, c);\n  return d;\n}\n\nfunction Na(a) {\n  return 0 < a.__internal.path.length && a.__internal.errorMap ? a.__internal.errorMap[a.__internal.path.join(\".\")] : void 0;\n}\n\nfunction Oa(a, b, c, d, e, f, g) {\n  a = {\n    store: a,\n    variables: b,\n    fragments: c,\n    parent: {\n      __typename: d\n    },\n    parentTypeName: d,\n    parentKey: e,\n    parentFieldKey: \"\",\n    fieldName: \"\",\n    error: void 0,\n    partial: !1,\n    optimistic: !!f,\n    __internal: {\n      path: [],\n      errorMap: void 0\n    }\n  };\n\n  if (g && g.graphQLErrors) {\n    for (b = 0; b < g.graphQLErrors.length; b++) {\n      (c = g.graphQLErrors[b]).path && c.path.length && (a.__internal.errorMap || (a.__internal.errorMap = Object.create(null)), a.__internal.errorMap[c.path.join(\".\")] = c);\n    }\n  }\n\n  return a;\n}\n\nfunction Pa(a, b, c, d, e, f) {\n  Ma.current = a;\n  a.parent = b;\n  a.parentTypeName = c;\n  a.parentKey = d;\n  a.parentFieldKey = e;\n  a.fieldName = f;\n  a.error = Na(a);\n}\n\nfunction Qa(a, b, c, d) {\n  if (!b) {\n    return !1;\n  }\n\n  var e = a.typeCondition ? n(a.typeCondition) : null;\n\n  if (!e || b === e) {\n    return !0;\n  }\n\n  \"production\" !== process.env.NODE_ENV && A(\"Heuristic Fragment Matching: A fragment is trying to match against the `\" + b + \"` type, but the type condition is `\" + e + \"`. Since GraphQL allows for interfaces `\" + e + \"` may be aninterface.\\nA schema needs to be defined for this match to be deterministic, otherwise the fragment will be matched heuristically!\", 16);\n  return !t(a).some(function (a) {\n    if (a.kind !== Kind.FIELD) {\n      return !1;\n    }\n\n    a = I(n(a), aa(a, d));\n    return !(void 0 !== V(c, a) || void 0 !== W(c, a));\n  });\n}\n\nfunction Ra(a, b, c, d) {\n  var e,\n      f = 0;\n  return function () {\n    var g, h, k, l;\n\n    if (void 0 !== e) {\n      if (void 0 !== (g = e())) {\n        return g;\n      }\n\n      e = void 0;\n      \"production\" !== process.env.NODE_ENV && u.pop();\n    }\n\n    for (; f < c.length;) {\n      if (ea(g = c[f++], d.variables)) {\n        if (g.kind === Kind.FIELD) {\n          return g;\n        }\n\n        if (void 0 !== (g = g.kind === Kind.INLINE_FRAGMENT ? g : d.fragments[n(g)])) {\n          if (d.store.schema) {\n            h = d.store.schema;\n\n            if (k = a) {\n              (l = g.typeCondition ? n(g.typeCondition) : null) && k !== l ? h.types[l] && \"OBJECT\" === h.types[l].kind ? h = l === k : (y(h.types[l] && (\"INTERFACE\" === h.types[l].kind || \"UNION\" === h.types[l].kind), \"production\" !== process.env.NODE_ENV ? \"Invalid Abstract type: The type `\" + l + \"` is not an Interface or Union type in the defined schema, but a fragment in the GraphQL document is using it as a type condition.\" : \"\", 5), pa(h, k), h = h.isSubType(l, k)) : h = !0;\n            } else {\n              h = !1;\n            }\n          } else {\n            h = Qa(g, a, b, d.variables);\n          }\n\n          if (h) {\n            return \"production\" !== process.env.NODE_ENV && x(a, g), (e = Ra(a, b, t(g), d))();\n          }\n        }\n      }\n    }\n  };\n}\n\nfunction Sa(a) {\n  return void 0 === a ? null : a;\n}\n\nfunction Ta(a, b, c, d, e) {\n  S(\"write\", a.data, e || null);\n  a = Ua(a, b, c, d);\n  T();\n  return a;\n}\n\nfunction Ua(a, b, c, d, e) {\n  var f = C(b.query),\n      g = {\n    data: c,\n    dependencies: ya()\n  },\n      h = a.rootFields[f.operation];\n  a = Oa(a, ba(f, b.variables), D(b.query), h, h, !!e, d);\n  \"production\" !== process.env.NODE_ENV && x(h, f);\n  Va(a, h, t(f), c);\n  \"production\" !== process.env.NODE_ENV && u.pop();\n  return g;\n}\n\nfunction Va(a, b, c, d) {\n  var h,\n      k,\n      l,\n      p,\n      m,\n      w,\n      v,\n      B,\n      e = b === a.store.rootFields.query,\n      f = !e && !!a.store.rootNames[b],\n      g = f || e ? b : d.__typename;\n\n  if (g) {\n    f || e || !b || X(b, \"__typename\", g);\n    c = Ra(g, b || g, c, a);\n\n    for (; h = c();) {\n      e = I(k = n(h), l = aa(h, a.variables));\n      m = d[p = h.alias ? h.alias.value : n(h)];\n\n      if (\"production\" !== process.env.NODE_ENV) {\n        if (f || void 0 !== m) {\n          if (a.store.schema && g && \"__typename\" !== k) {\n            w = a.store.schema, v = g;\n            ia.test(B = k) || F(w, v, B);\n          }\n        } else {\n          k = a.optimistic ? \"\\nYour optimistic result may be missing a field!\" : \"\";\n          l = void 0 === h.selectionSet ? \"scalar (number, boolean, etc)\" : \"selection set\";\n          \"production\" !== process.env.NODE_ENV && A(\"Invalid undefined: The field at `\" + e + \"` is `undefined`, but the GraphQL query expects a \" + l + \" for this field.\" + k, 13);\n          continue;\n        }\n      }\n\n      if (\"__typename\" !== k) {\n        a.__internal.path.push(p);\n\n        if (a.optimistic && f) {\n          if (!(m = a.store.optimisticMutations[k])) {\n            continue;\n          }\n\n          Pa(a, d, g, g, e, k);\n          m = d[p] = Sa(m(l || {}, a.store, a));\n        }\n\n        h.selectionSet ? b && !f ? (p = b + \".\" + e, Ha(b || g, e, h = Wa(a, t(h), Sa(m), p))) : Wa(a, t(h), Sa(m)) : b && !f && X(b || g, e, null === m && Na(a) ? void 0 : m);\n        f && (h = a.store.updates[g][k]) && (Pa(a, d, g, g, g + \".\" + e, k), d[k] = m, h(d, l || {}, a.store, a));\n\n        a.__internal.path.pop();\n      }\n    }\n  } else {\n    \"production\" !== process.env.NODE_ENV && A(\"Couldn't find __typename when writing.\\nIf you're writing to the cache manually have to pass a `__typename` property on each entity in your data.\", 14);\n  }\n}\n\nfunction Wa(a, b, c, d) {\n  var e, f, g, h;\n\n  if (Array.isArray(c)) {\n    for (e = Array(c.length), f = 0, g = c.length; f < g; f++) {\n      a.__internal.path.push(f);\n\n      h = Wa(a, b, c[f], d ? d + \".\" + f : void 0);\n      e[f] = h;\n\n      a.__internal.path.pop();\n    }\n\n    return e;\n  }\n\n  if (null === c) {\n    return Na(a) ? void 0 : null;\n  }\n\n  e = a.store.keyOfEntity(c);\n  f = c.__typename;\n  \"production\" !== process.env.NODE_ENV && (!d || a.store.keys[c.__typename] || null !== e || \"string\" != typeof f || Xa.test(f) || A(\"Invalid key: The GraphQL query at the field at `\" + d + \"` has a selection set, but no key could be generated for the data at this field.\\nYou have to request `id` or `_id` fields for all selection sets or create a custom `keys` config for `\" + f + \"`.\\nEntities without keys will be embedded directly on the parent entity. If this is intentional, create a `keys` config for `\" + f + \"` that always returns null.\", 15));\n  Va(a, d = e || d, b, c);\n  return d || null;\n}\n\nfunction Y(a) {\n  var b, c, d, e, f, g, h, k, l, p, m, w, v;\n  this.keyOfField = I;\n  this.resolveFieldByKey = this.resolve;\n  a || (a = {});\n  this.resolvers = a.resolvers || {};\n  this.optimisticMutations = a.optimistic || {};\n  this.keys = a.keys || {};\n  d = \"Query\", e = \"Mutation\", f = \"Subscription\";\n\n  if (a.schema) {\n    g = function ha(a) {\n      function b(a) {\n        switch (a.kind) {\n          case \"OBJECT\":\n          case \"INTERFACE\":\n            return {\n              name: a.name,\n              kind: a.kind,\n              interfaces: e(a.interfaces || []),\n              fields: e(a.fields.map(c))\n            };\n\n          case \"UNION\":\n            return {\n              name: a.name,\n              kind: a.kind,\n              types: e(a.possibleTypes || [])\n            };\n        }\n      }\n\n      function c(a) {\n        return {\n          name: a.name,\n          type: a.type,\n          args: e(a.args)\n        };\n      }\n\n      var d, e, f, g, h, k;\n      d = {}, e = fa, f = {\n        query: (a = a.__schema).queryType ? a.queryType.name : null,\n        mutation: a.mutationType ? a.mutationType.name : null,\n        subscription: a.subscriptionType ? a.subscriptionType.name : null,\n        types: void 0,\n        isSubType: function (a, b) {\n          var c = d[a],\n              e = d[b];\n          return c && e ? \"UNION\" === c.kind ? !!c.types[b] : \"OBJECT\" !== c.kind && \"OBJECT\" === e.kind ? !!e.interfaces[a] : a === b : !1;\n        }\n      };\n\n      if (a.types) {\n        f.types = d;\n\n        for (g = 0; g < a.types.length; g++) {\n          if ((h = a.types[g]) && h.name) {\n            (k = b(h)) && (d[h.name] = k);\n          }\n        }\n      }\n\n      return f;\n    }(a.schema);\n\n    d = g.query || d;\n    e = g.mutation || e;\n    f = g.subscription || f;\n    g.types && (this.schema = g);\n  }\n\n  this.updates = ((b = {})[e] = a.updates && a.updates.Mutation || {}, b[f] = a.updates && a.updates.Subscription || {}, b);\n  this.rootFields = {\n    query: d,\n    mutation: e,\n    subscription: f\n  };\n  this.rootNames = ((c = {})[d] = \"query\", c[e] = \"mutation\", c[f] = \"subscription\", c);\n  a = d;\n  this.data = {\n    defer: !1,\n    gc: new Set(),\n    persist: new Set(),\n    queryRootKey: a,\n    refCount: J(),\n    refLock: J(),\n    links: {\n      optimistic: J(),\n      base: new Map()\n    },\n    records: {\n      optimistic: J(),\n      base: new Map()\n    },\n    commutativeKeys: new Set(),\n    optimisticOrder: [],\n    storage: null\n  };\n\n  if (this.schema && \"production\" !== process.env.NODE_ENV) {\n    a = this.schema;\n    b = this.keys;\n\n    if (\"production\" !== process.env.NODE_ENV) {\n      for (h in b) {\n        \"production\" !== process.env.NODE_ENV && (a.types[h] || A(\"Invalid Object type: The type `\" + h + \"` is not an object in the defined schema, but the `keys` option is referencing it.\", 20));\n      }\n    }\n\n    h = this.schema;\n    a = this.updates;\n\n    if (\"production\" !== process.env.NODE_ENV) {\n      if (h.mutation) {\n        b = h.types[h.mutation].fields;\n        c = a[h.mutation] || {};\n\n        for (k in c) {\n          \"production\" !== process.env.NODE_ENV && void 0 === b[k] && A(\"Invalid mutation field: `\" + k + \"` is not in the defined schema, but the `updates.Mutation` option is referencing it.\", 21);\n        }\n      }\n\n      if (h.subscription) {\n        k = h.types[h.subscription].fields;\n        h = a[h.subscription] || {};\n\n        for (l in h) {\n          \"production\" !== process.env.NODE_ENV && void 0 === k[l] && A(\"Invalid subscription field: `\" + l + \"` is not in the defined schema, but the `updates.Subscription` option is referencing it.\", 22);\n        }\n      }\n    }\n\n    l = this.schema;\n    k = this.resolvers;\n\n    if (\"production\" !== process.env.NODE_ENV) {\n      for (p in k) {\n        if (\"Query\" === p) {\n          if (l.query) {\n            h = l.types[l.query].fields;\n\n            for (m in k.Query) {\n              h[m] || qa(\"Query.\" + m);\n            }\n          } else {\n            qa(\"Query\");\n          }\n        } else if (l.types[p]) {\n          if (\"INTERFACE\" === l.types[p].kind || \"UNION\" === l.types[p].kind) {\n            \"production\" !== process.env.NODE_ENV && A(\"Invalid resolver: `\" + p + \"` does not match to a concrete type in the schema, but the `resolvers` option is referencing it. Implement the resolver for the types that \" + (\"UNION\" === l.types[p].kind ? \"make up the union\" : \"implement the interface\") + \" instead.\", 26);\n          } else {\n            h = l.types[p].fields;\n\n            for (w in k[p]) {\n              h[w] || qa(p + \".\" + w);\n            }\n          }\n        } else {\n          qa(p);\n        }\n      }\n    }\n\n    m = this.schema;\n    p = this.optimisticMutations;\n\n    if (\"production\" !== process.env.NODE_ENV && m.mutation) {\n      m = m.types[m.mutation].fields;\n\n      for (v in p) {\n        \"production\" !== process.env.NODE_ENV && (m[v] || A(\"Invalid optimistic mutation field: `\" + v + \"` is not a mutation field in the defined schema, but the `optimistic` option is referencing it.\", 24));\n      }\n    }\n  }\n}\n\nfunction Za(a, b, c, d, e) {\n  S(\"read\", a.data, c && e || null);\n  a = Ya(a, b, c, d);\n  T();\n  return a;\n}\n\nfunction Ya(a, b, c, d) {\n  var e = C(b.query),\n      f = a.rootFields[e.operation],\n      g = t(e);\n  a = Oa(a, ba(e, b.variables), D(b.query), f, f, !1, d);\n  \"production\" !== process.env.NODE_ENV && x(f, e);\n  c = f !== a.store.rootFields.query ? $a(a, f, g, c || {}) : Z(a, f, g, {});\n  \"production\" !== process.env.NODE_ENV && u.pop();\n  return {\n    dependencies: ya(),\n    partial: a.partial || !c,\n    data: c || null\n  };\n}\n\nfunction $a(a, b, c, d) {\n  var f,\n      g,\n      e = a.store.rootNames[b] ? b : d.__typename;\n\n  if (\"string\" != typeof e) {\n    return d;\n  }\n\n  b = Ra(b, b, c, a);\n\n  for (e = {\n    __typename: e\n  }; c = b();) {\n    g = d[f = c.alias ? c.alias.value : n(c)];\n\n    a.__internal.path.push(f);\n\n    c.selectionSet && null !== g ? (g = Sa(g), e[f] = ab(a, t(c), g)) : e[f] = g;\n\n    a.__internal.path.pop();\n  }\n\n  return e;\n}\n\nfunction ab(a, b, c) {\n  if (Array.isArray(c)) {\n    for (var d = Array(c.length), e = 0, f = c.length; e < f; e++) {\n      a.__internal.path.push(e), d[e] = ab(a, b, c[e]), a.__internal.path.pop();\n    }\n\n    return d;\n  }\n\n  if (null === c) {\n    return null;\n  }\n\n  return null !== (d = a.store.keyOfEntity(c)) ? void 0 === (a = Z(a, d, b, {})) ? null : a : $a(a, c.__typename, b, c);\n}\n\nfunction Z(a, b, c, d, e) {\n  var k,\n      l,\n      p,\n      m,\n      w,\n      v,\n      B,\n      N,\n      r,\n      G,\n      E,\n      q,\n      z,\n      H,\n      f = a.store,\n      g = b === f.rootFields.query,\n      h = e && f.keyOfEntity(e) || b;\n  \"production\" !== process.env.NODE_ENV && !g && a.store.rootNames[h] && A(\"Invalid root traversal: A selection was being read on `\" + h + \"` which is an uncached root type.\\nThe `\" + a.store.rootFields.mutation + \"` and `\" + a.store.rootFields.subscription + \"` types are special Operation Root Types and cannot be read back from the cache.\", 25);\n\n  if (\"string\" == typeof (b = g ? b : V(h, \"__typename\") || e && e.__typename)) {\n    if (e && b !== e.__typename) {\n      \"production\" !== process.env.NODE_ENV && A(\"Invalid resolver data: The resolver at `\" + h + \"` returned an invalid typename that could not be reconciled with the cache.\", 8);\n    } else {\n      c = Ra(b, h, c, a);\n\n      for (l = !1, p = !1; void 0 !== (k = c());) {\n        m = n(k), w = aa(k, a.variables), v = k.alias ? k.alias.value : n(k), N = h + \".\" + (B = I(m, w)), r = V(h, B), G = e ? e[m] : void 0, E = f.resolvers[b];\n\n        if (\"production\" !== process.env.NODE_ENV && f.schema && b) {\n          q = f.schema, z = b;\n          ia.test(H = m) || F(q, z, H);\n        }\n\n        if (\"__typename\" === m) {\n          d[v] = b;\n        } else {\n          q = void 0;\n\n          a.__internal.path.push(v);\n\n          if (void 0 !== G && void 0 === k.selectionSet) {\n            q = G;\n          } else if (y(null !== K, \"production\" !== process.env.NODE_ENV ? \"Invalid Cache call: The cache may only be accessed or mutated duringoperations like write or query, or as part of its resolvers, updaters, or optimistic configs.\" : \"\", 2), \"read\" === K && E && \"function\" == typeof E[m]) {\n            if (Pa(a, d, b, h, N, m), void 0 !== r && (d[v] = r), q = E[m](d, w || {}, f, a), k.selectionSet && (q = bb(a, b, m, N, t(k), d[v], q)), f.schema && null === q && !na(f.schema, b, m)) {\n              return;\n            }\n          } else {\n            k.selectionSet ? void 0 !== G ? q = bb(a, b, m, N, t(k), d[v], G) : void 0 !== (w = W(h, B)) ? q = cb(a, w, b, m, t(k), d[v]) : \"object\" == typeof r && null !== r && (q = r) : q = r;\n          }\n\n          void 0 === q && Na(a) && (p = !0, q = null);\n\n          a.__internal.path.pop();\n\n          if (void 0 === q && f.schema && na(f.schema, b, m)) {\n            p = !0, d[v] = null;\n          } else {\n            if (void 0 === q) {\n              return;\n            }\n\n            l = !0;\n            d[v] = q;\n          }\n        }\n      }\n\n      p && (a.partial = !0);\n      return g && p && !l ? void 0 : d;\n    }\n  }\n}\n\nfunction bb(a, b, c, d, e, f, g) {\n  var h, k, l, p, m;\n\n  if (Array.isArray(g)) {\n    h = !(h = a.store).schema || oa(h.schema, b, c);\n\n    for (k = Array(g.length), l = 0, p = g.length; l < p; l++) {\n      a.__internal.path.push(l);\n\n      m = bb(a, b, c, d + \".\" + l, e, null != f ? f[l] : void 0, g[l]);\n\n      a.__internal.path.pop();\n\n      if (void 0 !== m || h) {\n        k[l] = void 0 !== m ? m : null;\n      } else {\n        return;\n      }\n    }\n\n    return k;\n  }\n\n  if (null == g) {\n    return g;\n  }\n\n  if (null === f) {\n    return null;\n  }\n\n  if (\"string\" == typeof g || \"object\" == typeof g && \"string\" == typeof g.__typename) {\n    return b = f || {}, \"string\" == typeof g ? Z(a, g, e, b) : Z(a, d, e, b, g);\n  }\n\n  \"production\" !== process.env.NODE_ENV && A(\"Invalid resolver value: The field at `\" + d + \"` is a scalar (number, boolean, etc), but the GraphQL query expects a selection set for this field.\", 9);\n}\n\nfunction cb(a, b, c, d, e, f) {\n  var g, h, k, l, p;\n\n  if (Array.isArray(b)) {\n    g = (g = a.store).schema && oa(g.schema, c, d);\n\n    for (h = Array(b.length), k = 0, l = b.length; k < l; k++) {\n      a.__internal.path.push(k);\n\n      p = cb(a, b[k], c, d, e, null != f ? f[k] : void 0);\n\n      a.__internal.path.pop();\n\n      if (void 0 !== p || g) {\n        h[k] = void 0 !== p ? p : null;\n      } else {\n        return;\n      }\n    }\n\n    return h;\n  }\n\n  return null === b || null === f ? null : Z(a, b, e, f || {});\n}\n\nfunction db(a, b) {\n  return makeOperation(a.kind, a, _extends({}, a.context, {\n    meta: _extends({}, a.context.meta, {\n      cacheOutcome: b\n    })\n  }));\n}\n\nfunction eb(a, b) {\n  return makeOperation(a.kind, a, _extends({}, a.context, {\n    requestPolicy: b\n  }));\n}\n\nfunction fb(a) {\n  return fromArray(a[0]);\n}\n\nfunction gb(a, b) {\n  return a.push(b), a;\n}\n\nfunction hb(a) {\n  return \"query\" === a.kind && \"network-only\" !== a.context.requestPolicy;\n}\n\nfunction ib(a) {\n  return \"query\" !== a.kind || \"network-only\" === a.context.requestPolicy;\n}\n\nfunction jb(a) {\n  return \"miss\" !== a.outcome || \"cache-only\" === a.operation.context.requestPolicy;\n}\n\nfunction kb(a) {\n  return function (b) {\n    function c(a, b) {\n      var k,\n          c = a.operation,\n          d = a.error,\n          f = a.extensions,\n          g = c.key;\n\n      if (\"mutation\" === c.kind) {\n        h(b, R.get(g));\n        R.delete(g);\n      } else {\n        ua(z.data, c.key);\n      }\n\n      if (a.data) {\n        if (h(b, Ta(z, c, a.data, a.error, g).dependencies), g = Za(z, c, a.data, a.error, g), a.data = g.data, \"query\" === c.kind) {\n          h(b, k = g.dependencies);\n        }\n      } else {\n        S(\"read\", z.data, c.key, void 0), T();\n      }\n\n      k && e(a.operation, k);\n      return {\n        data: a.data,\n        error: d,\n        extensions: f,\n        operation: c\n      };\n    }\n\n    function d(a) {\n      var b = Za(z, a),\n          c = b.data ? b.partial ? \"partial\" : \"hit\" : \"miss\";\n      e(a, b.dependencies);\n      return {\n        outcome: c,\n        operation: a,\n        data: b.data,\n        dependencies: b.dependencies\n      };\n    }\n\n    function e(a, b) {\n      for (var c in b) {\n        (ja[c] || (ja[c] = [])).push(a.key), ka.set(a.key, a);\n      }\n    }\n\n    function f(a) {\n      var b, c, d, e, f, k, l, O, m, H;\n\n      if (\"query\" === a.kind) {\n        ua(z.data, a.key);\n      } else if (\"teardown\" === a.kind) {\n        ka.delete(a.key), S(\"read\", z.data, a.key, void 0), T();\n      } else if (\"mutation\" === a.kind && \"network-only\" !== a.context.requestPolicy) {\n        b = z, c = a.key;\n        \"production\" !== process.env.NODE_ENV && y(\"mutation\" === C(a.query).operation, \"production\" !== process.env.NODE_ENV ? \"writeOptimistic(...) was called with an operation that is not a mutation.\\nThis case is unsupported and should never occur.\" : \"\", 10);\n        S(\"write\", b.data, c, !0);\n        b = Ua(b, a, {}, void 0, !0);\n        T();\n        b = b.dependencies;\n\n        a: {\n          for (d in b) {\n            d = !1;\n            break a;\n          }\n\n          d = !0;\n        }\n\n        if (!d) {\n          for (e in b) {\n            la[e] = !0;\n          }\n\n          R.set(a.key, b);\n          h(e = new Set(), b);\n          g(a, e);\n        }\n      }\n\n      e = makeOperation;\n      d = a.kind;\n      b = a.key;\n      c = formatDocument(a.query);\n\n      if (a.variables) {\n        f = C(a.query);\n\n        if ((k = a.variables) && f.variableDefinitions) {\n          for (l = {}, O = 0, m = f.variableDefinitions.length; O < m; O++) {\n            l[H = n(f.variableDefinitions[O].variable)] = k[H];\n          }\n\n          f = l;\n        } else {\n          f = void 0;\n        }\n      } else {\n        f = a.variables;\n      }\n\n      return e(d, {\n        key: b,\n        query: c,\n        variables: f\n      }, a.context);\n    }\n\n    function g(a, b) {\n      b.forEach(function (b) {\n        var c, d;\n\n        if (b !== a.key) {\n          if (c = ka.get(b)) {\n            ka.delete(b);\n            d = \"cache-first\";\n            za.has(b) && (za.delete(b), d = \"cache-and-network\");\n            E.reexecuteOperation(eb(c, d));\n          }\n        }\n      });\n    }\n\n    function h(a, b) {\n      var c, d, e;\n\n      if (b) {\n        for (c in b) {\n          if (b = ja[c]) {\n            ja[c] = [];\n\n            for (d = 0, e = b.length; d < e; d++) {\n              a.add(b[d]);\n            }\n          }\n        }\n      }\n    }\n\n    function k(a) {\n      for (var b in a) {\n        if (la[b]) {\n          return !0;\n        }\n      }\n\n      return !1;\n    }\n\n    function p(a) {\n      \"production\" !== process.env.NODE_ENV && q({\n        type: \"cacheMiss\",\n        message: \"The result could not be retrieved from the cache\",\n        operation: a.operation,\n        source: \"cacheExchange\"\n      });\n      return db(a.operation, \"miss\");\n    }\n\n    function m(a) {\n      return \"miss\" === a.outcome && \"cache-only\" !== a.operation.context.requestPolicy && !k(a.dependencies);\n    }\n\n    function w(a) {\n      var b = a.operation,\n          c = a.outcome,\n          d = a.dependencies,\n          e = {\n        operation: db(b, c),\n        data: a.data,\n        error: a.error,\n        extensions: a.extensions\n      };\n\n      if (\"cache-and-network\" === b.context.requestPolicy || \"cache-first\" === b.context.requestPolicy && \"partial\" === c) {\n        e.stale = !0, k(d) ? \"cache-and-network\" === b.context.requestPolicy && za.add(b.key) : E.reexecuteOperation(eb(b, \"network-only\"));\n      }\n\n      \"production\" !== process.env.NODE_ENV && q({\n        type: \"cacheHit\",\n        message: \"A requested operation was found and returned from the cache.\",\n        operation: a.operation,\n        data: {\n          value: e\n        },\n        source: \"cacheExchange\"\n      });\n      return e;\n    }\n\n    function v(a) {\n      var b = new Set(),\n          d = c(a, b);\n      g(a.operation, b);\n      return d;\n    }\n\n    function B(a) {\n      return !R.has(a.operation.key);\n    }\n\n    function N(a) {\n      var b, d, e;\n\n      if (ma.push(a) < R.size) {\n        return empty;\n      }\n\n      for (b = 0; b < ma.length; b++) {\n        ua(z.data, ma[b].operation.key);\n      }\n\n      for (d in la) {\n        delete la[d];\n      }\n\n      b = [];\n      d = new Set();\n\n      for (; e = ma.shift();) {\n        b.push(c(e, d));\n      }\n\n      g(a.operation, d);\n      return fromArray(b);\n    }\n\n    function r(a) {\n      return R.has(a.operation.key);\n    }\n\n    var H,\n        R,\n        ma,\n        ka,\n        la,\n        za,\n        ja,\n        G = b.forward,\n        E = b.client,\n        q = b.dispatchDebug,\n        z = new Y(a);\n    a && a.storage && (H = a.storage.readData().then(function l(b) {\n      var e,\n          f,\n          g,\n          h,\n          c = z.data,\n          d = a.storage;\n      S(\"write\", c, null);\n\n      for (e in b) {\n        if (void 0 !== (f = b[e])) {\n          h = (g = ta(e)).entityKey;\n          g = g.fieldKey;\n          \":\" === f[0] ? Ha(h, g, JSON.parse(f.slice(1))) : X(h, g, JSON.parse(f));\n        }\n      }\n\n      T();\n      c.storage = d;\n    }));\n    R = new Map(), ma = [], ka = new Map(), la = J(), za = new Set(), ja = J();\n    return function (a) {\n      var b, c;\n      a = share(a);\n      b = H ? mergeMap(fb)(take(1)(combine(scan(gb, [])(a), fromPromise(H)))) : empty;\n      b = share(concat([b, a]));\n      a = share(map(d)(filter(hb)(b)));\n      b = filter(ib)(b);\n      c = map(p)(filter(m)(a));\n      a = map(w)(filter(jb)(a));\n      c = share(G(map(f)(merge([b, c]))));\n      b = map(v)(filter(B)(c));\n      c = mergeMap(N)(filter(r)(c));\n      return merge([b, c, a]);\n    };\n  };\n}\n\nfunction lb(a) {\n  return a && a.networkError && !a.response && (\"undefined\" != typeof navigator && !1 === navigator.onLine || /request failed|failed to fetch|network\\s?error/i.test(a.networkError.message));\n}\n\nfunction offlineExchange(a) {\n  return function (b) {\n    function e(a) {\n      var b, c, d, e, f;\n\n      if (b = \"mutation\" === a.operation.kind && lb(a.error)) {\n        a: {\n          b = N;\n          d = (c = a.operation).variables || J(), e = D(c.query);\n          c = [].concat(t(C(c.query)));\n\n          for (; f = c.pop();) {\n            if (ea(f, d)) {\n              if (f.kind !== Kind.FIELD) {\n                (f = f.kind === Kind.INLINE_FRAGMENT ? f : e[n(f)]) && c.push.apply(c, t(f));\n              } else if (b[n(f)]) {\n                b = !0;\n                break a;\n              }\n            }\n          }\n\n          b = !1;\n        }\n      }\n\n      return b ? (r.push(a.operation), G(), !1) : !0;\n    }\n\n    function h(a) {\n      return \"query\" === a.operation.kind && lb(a.error) ? (B(eb(a.operation, \"cache-only\")), r.push(a.operation), !1) : !0;\n    }\n\n    var p,\n        m,\n        w,\n        v,\n        B,\n        N,\n        r,\n        G,\n        E,\n        q,\n        z,\n        l = a.storage;\n\n    if (l && l.onOnline && l.readMetadata && l.writeMetadata) {\n      p = b.forward, m = b.client;\n      b = b.dispatchDebug;\n      w = makeSubject(), v = w.source, B = w.next, N = a.optimistic || {}, r = [], G = function c() {\n        var a, b, c;\n\n        for (a = [], b = 0; b < r.length; b++) {\n          \"mutation\" === (c = r[b]).kind && a.push({\n            query: print(c.query),\n            variables: c.variables\n          });\n        }\n\n        l.writeMetadata(a);\n      }, E = !1;\n      l.onOnline(q = function d() {\n        var a, b;\n\n        if (!E) {\n          E = !0;\n\n          for (a = 0; a < r.length; a++) {\n            \"mutation\" === (b = r[a]).kind && B(makeOperation(\"teardown\", b));\n          }\n\n          for (a = 0; a < r.length; a++) {\n            m.reexecuteOperation(r[a]);\n          }\n\n          r.length = 0;\n          E = !1;\n          G();\n        }\n      });\n      l.readMetadata().then(function g(a) {\n        if (a) {\n          for (var b = 0; b < a.length; b++) {\n            r.push(m.createRequestOperation(\"mutation\", createRequest(a[b].query, a[b].variables)));\n          }\n\n          q();\n        }\n      });\n      z = kb(a)({\n        client: m,\n        dispatchDebug: b,\n        forward: function f(a) {\n          return filter(e)(p(a));\n        }\n      });\n      return function k(a) {\n        a = share(a);\n        a = merge([v, a]);\n        return filter(h)(z(a));\n      };\n    }\n\n    return kb(a)(b);\n  };\n}\n\nvar ca, u, ia, K, L, M, P, Q, Ma, Xa;\nimport { Kind } from \"graphql/language/kinds.mjs\";\nimport { valueFromASTUntyped } from \"graphql/utilities/valueFromASTUntyped.mjs\";\nimport { print } from \"graphql/language/printer.mjs\";\nimport { _ as _extends } from \"./5301ccd2.mjs\";\nimport { share, mergeMap, take, combine, scan, fromPromise, empty, concat, map, filter, merge, fromArray, makeSubject } from \"wonka\";\nimport { stringifyVariables, createRequest, formatDocument, makeOperation } from \"@urql/core\";\nca = new Set(), u = [];\nia = /^__/;\nK = null, L = null, M = null, P = null, Q = !1;\nMa = {\n  current: null\n};\nXa = /^__|PageInfo|(Connection|Edge)$/;\n\nY.prototype.keyOfEntity = function (a) {\n  if (Ma.current && a === Ma.current.parent) {\n    return Ma.current.parentKey;\n  }\n\n  if (null == a || \"string\" == typeof a) {\n    return a || null;\n  }\n\n  if (!a.__typename) {\n    return null;\n  }\n\n  if (this.rootNames[a.__typename]) {\n    return a.__typename;\n  }\n\n  var b;\n  this.keys[a.__typename] ? b = this.keys[a.__typename](a) : null != a.id ? b = \"\" + a.id : null != a._id && (b = \"\" + a._id);\n  return b ? a.__typename + \":\" + b : null;\n};\n\nY.prototype.resolve = function (a, b, c) {\n  b = I(b, c);\n\n  if (!(a = this.keyOfEntity(a))) {\n    return null;\n  }\n\n  return void 0 !== (c = V(a, b)) ? c : W(a, b) || null;\n};\n\nY.prototype.invalidate = function (a, b, c) {\n  var e,\n      d = this.keyOfEntity(a);\n  y(d, \"production\" !== process.env.NODE_ENV ? \"object\" == \"Can't generate a key for invalidate(...).\\nYou have to pass an id or _id field or create a custom `keys` field for `\" + typeof a ? a.__typename : a + \"`.\" : \"\", 19);\n  a = b ? [{\n    fieldKey: I(b, c)\n  }] : La(d);\n  b = 0;\n\n  for (c = a.length; b < c; b++) {\n    void 0 !== W(d, e = a[b].fieldKey) ? Ha(d, e, void 0) : X(d, e, void 0);\n  }\n};\n\nY.prototype.inspectFields = function (a) {\n  return (a = this.keyOfEntity(a)) ? La(a) : [];\n};\n\nY.prototype.updateQuery = function (a, b) {\n  (a = createRequest(a.query, a.variables)).query = formatDocument(a.query);\n  null !== (b = b(this.readQuery(a))) && Ua(this, a, b);\n};\n\nY.prototype.readQuery = function (a) {\n  (a = createRequest(a.query, a.variables)).query = formatDocument(a.query);\n  return Ya(this, a).data;\n};\n\nY.prototype.readFragment = function (a, b, c) {\n  var d, e;\n\n  if (d = (a = D(a = formatDocument(a)))[(d = Object.keys(a))[0]]) {\n    e = d.typeCondition.name.value;\n    \"string\" == typeof b || b.__typename || (b.__typename = e);\n    (b = this.keyOfEntity(b)) ? (\"production\" !== process.env.NODE_ENV && x(e, d), c = Z(c = Oa(this, c || {}, a, e, b), b, t(d), {}) || null, \"production\" !== process.env.NODE_ENV && u.pop()) : (\"production\" !== process.env.NODE_ENV && A(\"Can't generate a key for readFragment(...).\\nYou have to pass an `id` or `_id` field or create a custom `keys` config for `\" + e + \"`.\", 7), c = null);\n  } else {\n    \"production\" !== process.env.NODE_ENV && A(\"readFragment(...) was called with an empty fragment.\\nYou have to call it with at least one fragment in your GraphQL document.\", 6), c = null;\n  }\n\n  return c;\n};\n\nY.prototype.writeFragment = function (a, b, c) {\n  var d, e, f;\n\n  if (d = (a = D(a = formatDocument(a)))[(d = Object.keys(a))[0]]) {\n    b = _extends({}, {\n      __typename: e = d.typeCondition.name.value\n    }, b);\n    (f = this.keyOfEntity(b)) ? (\"production\" !== process.env.NODE_ENV && x(e, d), Va(c = Oa(this, c || {}, a, e, f, void 0), f, t(d), b), \"production\" !== process.env.NODE_ENV && u.pop()) : \"production\" !== process.env.NODE_ENV && A(\"Can't generate a key for writeFragment(...) data.\\nYou have to pass an `id` or `_id` field or create a custom `keys` config for `\" + e + \"`.\", 12);\n  } else {\n    \"production\" !== process.env.NODE_ENV && A(\"writeFragment(...) was called with an empty fragment.\\nYou have to call it with at least one fragment in your GraphQL document.\", 11);\n  }\n};\n\nexport { Y as Store, kb as cacheExchange, offlineExchange, Za as query, Ta as write };","map":{"version":3,"sources":["../src/ast/variables.ts","../src/helpers/help.ts","../src/ast/traversal.ts","../src/ast/schema.ts","../src/ast/schemaPredicates.ts","../src/store/data.ts","../src/operations/write.ts","../src/operations/shared.ts","../src/ast/node.ts","../src/store/store.ts","../src/operations/invalidate.ts","../src/operations/query.ts","../src/store/keys.ts","../src/helpers/operation.ts","../src/cacheExchange.ts","../src/offlineExchange.ts"],"names":["arg","aa","i","node","vars","input","a","typename","currentDebugStack","doc","invariant","name","getName","definitions","type","abstractType","let","schema","BUILTIN_FIELD_RE","ha","getField","field","validQueries","warnAboutResolver","validTypeProperties","e","currentOptimisticKey","makeDict","types","data","initDataState","map","undefined","fieldKey","entity","skip","optimistic","newCount","refCount","process","count","gc","updateRCForEntity","link","entityKey","extractNodeFields","currentData","linkNode","const","links","setNode","updateRCForLink","layerKey","previousDependencies","currentDependencies","writeLink","keyMap","deleteLayer","updateDependencies","extractNodeMapFields","currentOptimistic","b","startWrite","request","clearDataState","Y","normalizeVariables","fragments","Object","isQuery","fieldName","fieldAlias","fieldValue","ctx","updateContext","isRoot","getSelectionSet","InMemoryData","writeField","KEYLESS_TYPE_RE","childKey","writeSelection","delete","store","d","error","U","keyOfField","index","select","isInterfaceOfType","x","Kind","queryName","subscriptionName","resolvers","key","Ua","createRequest","formatDocument","updater","read","fields","l","pushDebugNode","rootKey","rootSelect","k","dependencies","originalData","newData","isFieldNullable","hasFields","Array","resolveResolverResult","prevData","h","isListNullable","args","dotIndex","operation","makeOperation","opts","hydration","entries","deps","pendingOperations","op","ops","toRequestPolicy","optimisticKeysToDependencies","filterVariables","reserveLayer","write","queryDependencies","result","sharedOps$","share","bufferedOps$","acc","concat","filter","cacheMissOps$","addCacheOutcome","optimisticMutationCompletion$","blockedDependencies","dep","getMainOperation","shouldInclude","storage","failedQueue","res","flushQueue","cacheResults$"],"mappings":";;;;;yCAsBqBA,U,GAAAA,E;;;SAMDC,E,CAAAA,C,EAAAA,C,EAAAA;;;;;;;;MA2BdK,CAAAA,CAAAA,SAAAA,IAAAA,CAAAA,CAAAA,SAAAA,CAAAA,M,EAAAA;;;;;;;;;;;;;MCSJE,C;MAAAA,C;MAAAA,CAAAA,GAAAA,E;;;WAnB4BD,C;;;QAc1BC,mB,EAAAA;;QAKFA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,mBAAAA,CAAAA,CAAAA,CAAAA,EAAAA,QAAAA,C,IAAAA,KAAAA,CAAAA,KAAAA,CAAAA,CAAAA,CAAAA,CAAAA,IAAAA,CAAAA,CAAAA,YAAAA,GAAAA,mBAAAA,CAAAA,CAAAA,CAAAA,YAAAA,EAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,C;;;;;;;;;;;;MCzCAE,CAAAA,GAAAA,E;;;;;AAaMP,SAAAA,EAAAA,GAAAA;;;;;;;;;;cAiByBD,C,EAAAA;KACvBS,G,CAAOC,C,MAAAA,OAAAA,CAAAA,IAAAA,CAAAA,CAAAA,GAAAA,EAAAA,EAAAA,GAAAA,2BAAAA,GAAAA,CAAAA,GAAAA,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,C;;;;;UAeRC,W,CAAAA,C,EAAAA,I,KAAAA,IAAAA,CAAAA,oB,EAAAA;;;;;;;;;;;;SCPGC,CAAAA,CAAAA,WAAAA,CAAAA,CAAAA,C,EAAAA,I,KAAAA,IAAAA,CAAAA,mB,KAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,C;;;;;;;MAkCDC,C;MAAAA,C;MAAAA,C;MAAAA,CAAAA,GAAAA,CAAAA,CAAAA,U;;;;;;OAAAA,CAAAA,GAAAA,CAAAA,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,M,EAEMA,CAAAA,GAAAA,C,EAAAA,CAAAA,E,EAAAA;SAeRC,eAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,KAAAA,WAAAA,C,KAAAA,CAAAA,CAAAA,S,IAAAA,CAAAA,CAAAA,SAAAA,CAAAA,CAAAA,C,IAAAA,SAAAA,CAAAA,CAAAA,CAAAA,CAAAA,SAAAA,CAAAA,CAAAA,CAAAA,C,EAAAA;;;;;;;;SASAC,E,CAAAA,C,EAAAA;;AC7GHC,IAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,IAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA;;;;;;SCgHEhB,E,CAAAA,C,EAAAA,C,EAAAA,C,EAAAA;SAMU2B,I,CAAAA,C,GAAAA;;;;qDAYwBA,I;;;AAoCxCnB,SAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;YAmCIqB,C;;;yBAYUC,eAAAA,CAAAA,CAGZE,IAHYF,CAGZE,IAHYF,GAGZE,CAAAA,CAAAA,IAAAA,CAFcD,MADFD,GACEC,CAAAA,CAAAA,I,EAAAA,I,IAAAA,eAAAA,CAAAA,CAAAA,MAAAA,CAAAA,I;;;;KAyBdE,C,EAAOA,C;QAGLC,K,CAAAA,C,EAAAA,M,CAAAA,C;;;;;SAcYJ,E,CAAAA,C,EAAT7B,C,EAAAA;;;;AAaDkC,SAAYC,EAAZD,CAAYC,CAAZD,EAAYC;mBAIAC,OAAAA,CAAAA,GAAAA,CAAAA,Q,IAAAA,CAAAA,CACPC,wBAA4BC,CAA5BD,GAA4BC,+EADrBF,EACqBE,EADrBF,C;;;;aAagBI,CAAAA,GAAhCD,GAAgCC,GAAhCD,kBAAAA,CAAAA,CAAAA,CAAgCC,GAAhCD,G,GAAAA,C;;;;YAGQE,O,CAAAA,G;;;;;;;;;;;;;;;;YAsCWb,C,EAAAA;oBAEjBc,G;;;;;;;;;;;WAmCKG,C,EAAAA,C,EAAAA,C,EAAAA,C,EAAAA;;;;;;WAOiBJ,IAAAA,CAAAA,CAAAA,eAAAA,CAAAA,M,IAAAA,CAAAA,IAAAA,CAAAA,CAAAA,eAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,IAAAA,CAAAA,CAAAA,eAAAA,CAAAA,MAAAA,CAAAA,CAAAA,CAAAA,GAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,E,KAAAA,E,mEAAAA,E;;;;;;MA0ClBE,CAAAA,GAAAA,C;;;;MAoBJpB,CAAAA,IAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,eAAAA,CAAAA,OAAAA,CAAAA,CAAAA,C,EAAAA;aAIAG,CAAAA,CAAAA,eAAAA,CAAAA,M,EACcH,KAAAA,EAAAA,CAAAA,IAAAA,CAAAA,CAAAA,OAAAA,CAAAA,CAAAA,CAAAA,eAAAA,CAAAA,CAAAA,CAAAA,CAAAA,IAAAA,CAAAA,CAAAA,eAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,eAAAA,CAAAA,CAAAA,CAAAA,C,GAAAA;;;;;;;qBASYuB,G,CAAAA,Q,IAAAA,CAAAA,CAAAA,K,KAAAA,CAAAA,CAAAA,KAAAA,GAAAA,CAAAA,CAAAA,EAAAA,OAAAA,CAAAA,OAAAA,GAAAA,IAAAA,CAAAA,SAAAA,CAAAA,GAAAA;;;;;;;gBCvN1ByB,CAAAA,CAAAA,S;gDAQkBC,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,MAAAA,kBAAAA,CAIdC,CAJcD,C,GAIdC,KAAAA,CAAAA,MAIFC,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAJED,IAIFC,CAAAA,CAAAA,CAAAA,CAAAA,GAEAC,kBAAAA,CAEOlC,CAFPkC,CANEF,GAQkBf,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,KAEtBgB,C;;;;;;;;;AAmCAE,QAAAA,CAAAA,CAAAA,OAAAA,CAAAA,SAAAA,CAAAA,CAAAA;;;;ADgHenC,IAAAA,CAAAA;;GA8CWK,C;;;AAO9BC,SAAAA,EAAAA,GAAAA;IAEAC,SAAAA,C,EAAAA,iBAAAA,OAAAA,CAAAA,GAAAA,CAAAA,QAAAA,GAAAA,mKAAAA,GAAAA,E,EAAAA,C;SAEAA,C;;;;uCASEtB,I,EAAAA,G,CAAAA,C;;;;;YAqCiBA,C,EAAoBuB,C,EAAAA,C,EAAAA;MACzBvB,C,EAAAA,C,EAAAA,C,EAAAA,C,EAAAA,C,EAMGuB,C;;4BANHvB,C,IAAAA,C,IAAAA,CAAAA,CAAAA,eAAAA,CAAAA,GAAAA,CAAAA,CAAAA,C,EAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,eAAAA,CAAAA,M,EAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,E,EAAAA;UAEZA,e,CAAAA,C;;;YAIeuB,CAAAA,GAAAA,CAAAA,CAAAA,UAAAA,CAAAA,CAAAA,C,KAAAA,CAAAA,IAAAA,CAAAA,CAAAA,eAAAA,CAAAA,GAAAA,CAAAA,CAAAA,C,IAAAA,CAAAA,IAAAA,YAAAA,CAAAA,IAAAA,CAAAA,CAAAA,CAAAA,eAAAA,CAAAA,GAAAA,CAAAA,CAAAA,C,KAAAA,KAAAA,CAAAA,MAAAA,CAAAA,GAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,C,IAAAA,CAAAA,IAAAA,C,EAAAA;AAaXG,aAA+BC,CAAAA,CAAOvB,CAAPuB,CAA/BD;;;;cAQuBtB,C,MAAAA,CAAAA,GAAAA,CAAAA,CAAAA,IAAAA,CAAAA,GAAAA,CAAAA,CAAAA,C,IAAAA,CAAAA,CAAAA,CAAAA,C,GAAAA,KAAAA,C;;;YAjBVmB,C,EAAAA,C,EAAAA,C,EAAAA,C,EAAAA;MAEbC,CAAAA,GAAAA,KAAuBC,CAAvBD,KAAuBC,CAAAA,CAAAA,CAAAA,CAAvBD,GAAuBC,CAAAA,CAAAA,CAAAA,CAAvBD,GAAuBC,C;MACP3B,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,GAAAA,CAAAA,GAAAA,C;4BAERmB,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,C,GAAAA,KAAAA,CAAAA,IAAAA,IAAAA,CAAAA,IAAAA,CAAAA,CAAAA,MAAAA,CAAAA,CAAAA,C;;;AAgBdQ,SAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;AAAAA,MAAAA,CAAAA,EAAAA,CAAAA,EAaAK,CAbAL;;MACAG,YAAAA,OAAAA,C,EAAAA;;;AASAC,SAAAA,CAAAA,GAAAA,CAAAA,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,MAAAA,EAAAA,CAAAA,GAAAA,CAAAA,EAAAA,CAAAA,EAAAA,EAAAA;OAGAC,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,C,KAAAA,EAAAA,CACAA,CADAA,EACAA,CADAA,EACAA,CADAA,EACAA,CADAA,C;;;;;;;;;;;;;AAUEb,SAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;AAAAA,MAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA;KAYAc,C,EAAoBC,C,EAAAA,CAAAA,CAAAA,IAAAA,CAAAA,GAAAA,CAAAA,CAAAA,C;;;;;;;;ME3fPuB,C,EAAAA,C;;;MAnBMF,M,CACnBC,C;;SAkBaC,C,IAAAA,CAAAA,CAAOC,O,EAAAA;oBACGA,CAAAA,CAAAA,OAAAA,CAAAA,CAAAA,C,EAAAA,C,KAAAA,C,GAAAA;;;;eAoBJZ,C;;;aAGrBA,Q,CAAAA,C;;;;;;;;;;;;;;SA8BOa,C,CAACV,C,EAAAA,C,EAAAA;mBClGMgB,C,KAAAA,CAAAA,KAAAA,CAAAA,CAAAA,YAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,GAAAA,KAAAA,CAAAA,KAAAA,CAAAA,KAAAA,CAAAA,CAAAA,CAAAA,GAAAA,GAAAA,GAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,C;;;ADoGN3D,SAAWsD,CAAXtD,CAAsBrB,CAAtBqB,EAAWsD,CAAXtD,EAAWsD;;;;;SAgBfC,C,CAAAA,C,EAAAA,C,EAAAA;;YAGoBxD,CAAAA,CAAAA,K,EAAW1B,C,EAAAA,C;;;;;UAMf0B,O,IAAAA,CAAAA,CAAAA,OAAAA,CAAAA,GAAAA,CAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,C;;;;eAOHyD,C,EAAAA,C,EAAAA;;;;;;;QAK4BtF,CAAAA,CAAAA,OAAAA,CAAAA,CAAAA,MAAAA,CAAAA,CAAAA,OAAAA,CAAAA,CAAAA,IAAAA,CAAAA,EAAAA,C;QAIlB6B,CAAAA,CAAAA,KAAAA,CAAAA,UAAAA,CAAAA,CAAAA,C;;;gBAEf0D,I;;;;;;;;;;;;;;;;;;YA4BaC,C,EAAAA,C,EAAAA;4BACzBA,O,CAAAA,C,GAAAA,OAAAA,CAAAA,CAAAA,OAAAA,CAAAA,UAAAA,CAAAA,CAAAA,C,EAAAA,OAAAA,CAAAA,CAAAA,KAAAA,CAAAA,UAAAA,CAAAA,CAAAA,C;;;ADhJA7D,SAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA;MACegC,CAAAA,GAAAA,CAAAA,CAAkBC,eAAlBD,CAAkBC,OAAlBD,CAAkBC,CAAlBD,C;GACfE,C,GAAAA,C,KAAAA,CAAAA,CAAAA,eAAAA,CAAAA,MAAAA,CAAAA,CAAAA,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,eAAAA,CAAAA,MAAAA,CAAAA,CAAAA,C;;;;YA+BmCD,C,EAAAA,C,EAAAA;gBAMjCG,C,EAAAA;;;;;;;MA6EIG,C,EAAAA,C,EAAAA,CAAAA,CAAAA,CAAAA,C;;;;;;UAuBEC,C;;OAGAC,CAAAA,CAAAA,KAAAA,CAAAA,UAAAA,CAAAA,CAAAA,C,KAAAA,CAAAA,CAAAA,OAAAA,CAAAA,EAAAA,C;iBAI0BvC,U,CAAfwC,C,MAAAA,CAAAA,CAAAA,OAAAA,CAAAA,EAAAA,C;;;;;;;;;;;;;SA8BjBC,C;;;;;;;AA2FK,SAAA,EAAA,CAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA;MAIWA;YAAAA;gBAAAA;gBAAAA;YA2BZO;AACNC,MAAAA,UAAAA,EAAAA;AADMD,KA3BYP;qBAAAA;gBAAAA;sBAAAA;iBAAAA;iBAAAA;eAAAA;mBAAAA;;;AG5QZoB,MAAAA,QAAAA,EAAAA,KAAAA;;AH4QYpB,G;;;SGrQKxD,CAAAA,GAAAA,C,EAAAA,CAAAA,GAAAA,CAAAA,CAAnB6E,aAAmB7E,CAAnB6E,M,EAAAA,CAAAA,E,EAAAA;;;;;;;;qBAwB0CV,C,EAAAA,C,EAAAA,C,EAAAA;KAClBnE,O,GAAAA,C;;;;;;;;;;;;;;;;;YL0DxBA,C;;;;;;;;;QKxDwC8E,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,C;;;;;ALwGhCzE,SAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;;;;;;0BAQNC,CAAAA,E,GAAAA;eAGGN,C;;;;mCAWGO,Q,IAAAA,CAAAA,CAAAA,GAAAA,E;;;;;;iBAVND,C;;;AKlHAN,YAAAA,KAAAA,CAAAA,MAAAA,CAAAA,GAAAA,CAAAA,CAAAA,IAAAA,KAAAA,IAAAA,CAAAA,eAAAA,GAAAA,CAAAA,GAAAA,CAAAA,CAAAA,SAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA;;;;;;;;;;;;;;;;;;;;;;AAoBA,SAAA,EAAA,CAAUY,CAAV,EAAUA;cAQPmE,C,KAAAA,C,GAAAA,I,GAAAA,C;;;AAQD/D,SAAWsD,EAAXtD,CAAWsD,CAAXtD,EAAWsD,CAAXtD,EAAWsD,CAAXtD,EAAWsD,CAAXtD,EAAWsD,CAAXtD,EAAWsD;;MAEbU,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,C;;;;;;;MAcErD,CAAAA,GAAY;AAGhBA,IAAAA,IAAAA,EAAAA,CAHgB;;AAAA,G;YCvIA0D,U,CAAAA,CAAAA,CAAmBC,S;YAChBD,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,SAAAA,C,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,KAAAA,C,EAAAA,C,EAAAA,C,EAAAA,CAAAA,CAAAA,C,EAAAA,C;2BAEnBzB,G,CAAkC5C,Q,IAAAA,CAAAA,CAElC4C,CAFkC5C,EAElC4C,CAFkC5C,C;;;;;;;;;;MC2KhCkC,C;MACQC,C;MAAAA,C;MAAAA,C;MAAAA,C;MFfI8B,CAAAA,GAAAA,CAAAA,KAAAA,CAAAA,CAAAA,KAAAA,CAAAA,UAAAA,CAAAA,K;MAAAA,CAAAA,GAAAA,CAAAA,CAAAA,IAAAA,CAAAA,CAAAA,CAAAA,CAAAA,KAAAA,CACAC,SADAD,CACAC,CADAD,C;MAEDE,CAAAA,GAAAA,CAAAA,IAAAA,CAAAA,GAAAA,CAAAA,GAAuBrC,CAAAA,CAAvBqC,U;;;;mBAOCF,C,EAAAA,C,EAAAA,C;;WAETG,CAAAA,GAAAA,CAAAA,E,GAAAA;AAULF,MAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,GAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,SAAAA,CAAAA,CAAAA;AENU/B,MAAAA,CAAAA,GAAAA,CAAAA,CADRD,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,KAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CACQC,CAAAA;;2BH/KdjE,OAAAA,CAAAA,GAAAA,CAAAA,Q,EAAAA;sBG4LI,C,KAAA,C,EAAA;yDAeAoC;;;;;;AL7FE4B,UAAAA,CAAAA,GAAAA,KAAAA,CAAAA,KAAAA,CAAAA,CAAAA,YAAAA,GAAAA,+BAAAA,GAAAA,eAAAA;AACQC,2BAAAA,OAAAA,CAAAA,GAAAA,CAAAA,QAAAA,IAAAA,CAAAA,CAAAA,sCAAAA,CAAAA,GAAAA,oDAAAA,GAAAA,CAAAA,GAAAA,kBAAAA,GAAAA,CAAAA,EAAAA,EAAAA,CAAAA;AACGD;;;;;;;;AK/DjBrC,cAAAA,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,mBAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA;AAC2BiC;;;;qBAS3BsB,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,IAAAA,EAAAA,EAAAA,CAAAA,CAAAA,KAAAA,EAAAA,CAAAA,CAAAA,C;;;4CAiBEmB,G,GAAAA,C,EAAAA,EAAAA,CAAAA,CAAAA,IAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,GAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,EAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,C,IAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,EAAAA,CAAAA,CAAAA,CAAAA,C,GAAAA,CAAAA,IAAAA,CAAAA,CAAAA,IAAAA,CAAAA,CAAAA,CAAAA,IAAAA,CAAAA,EAAAA,CAAAA,EAAAA,SAAAA,CAAAA,IAAAA,EAAAA,CAAAA,CAAAA,CAAAA,GAAAA,KAAAA,CAAAA,GAAAA,CAAAA,C;6CASkBC,EAAAA,CAASC,CAATD,EAAqBpG,CAArBoG,EAAqBpG,CAArBoG,EAAqBpG,CAArBoG,EAAqBpG,CAAAA,GACrBoE,GADqBpE,GACPqG,CADdD,EAC0BE,CAD1BF,CAAAA,EAC0BE,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAD1BF,EAC0BE,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,IAAAA,EAAAA,EAAAA,CAAAA,CAAAA,KAAAA,EAAAA,CAAAA,C;;UAO5CC,U,CAAAA,I,CAAAA,G;;;;;;;;AA2BAnC,SAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;AAAAA,MAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA;;;qBASAA,M,GAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,M,EAAAA,CAAAA,GAAAA,C,EAAAA,CAAAA,E,EAAAA;;;;;;wBAY0BoC,G;;;WAUnBC,C;;;;;;;;;mBAmFHzC,OAAAA,CAAAA,GAAAA,CAAAA,Q,KAAAA,CAAAA,CAAAA,IAAAA,CAAAA,CAAAA,KAAAA,CAEuBc,IAFvBd,CAEuBc,CAAAA,CAAAA,UAFvBd,CAAAA,IAEuBc,SAAAA,CAFvBd,IAEuBc,YAAAA,OAAAA,CAFvBd,IAEuBc,EAAAA,CAAAA,IAAAA,CAAAA,CAAAA,CAFvBd,IAEuBc,CAAAA,CAAAA,qDAAAA,CAAAA,GAAAA,0LAAAA,GAAAA,CAAAA,GAAAA,gIAAAA,GAAAA,CAAAA,GAAAA,6BAAAA,EAAAA,EAAAA,C;oBAgBZtB,C,EACbgB,C;;;;AAMG,SAAA,CAAA,CAAA,CAAA,EAAA;AAAA,MAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAAA,CAAA,EAmPDJ,CAnPC,EAmPDA,CAnPC,EAmPDA,CAnPC,EAmPDA,CAnPC,EAmPDA,CAnPC;;gCAoBqCA,O;YAQb,E;mBACJU,CAAAA,CAAAA,SAAAA,IAAAA,E;6BPtQD5E,CAAAA,CAAAA,UAAAA,IAAAA,E;;;;;iBArBnBY,E,CAAAA,C,EAAAA;;gBAIPF,CAAAA,CAAAA,I;AAIcG,eAAiBb,QAAjBa;AACV,eAAA,WAAA;;;oBAEFC,CAAAA,CAAAA,I;;;;;AAqCaJ,eAAAA,OAAAA;;;;;;;;;;eQnEUqD;kBACzBgD,IADyBhD;sBAAAA;gBAMIrC,CAAAA,CAAAA,CAAAA,CAAAA,IAAAA;AANJqC,S;;;yBP+IrBxC,C;UOvHDc,E,EAAAA,CAAAA,GAAAA,E,EAAAA,CAAAA,GAAAA;wDAIeoD,I,GAAAA,IAJfpD;kBAKwB2E,CAAAA,CAAAA,YAAAA,GAAAA,CAAAA,CAAAA,YAAAA,CAAAA,IAAAA,GAAAA,IALxB3E;wBAMMA,gB,GAAAA,CAAAA,CAAAA,gBAAAA,CAAAA,I,GAAAA,IANNA;qBAAAA;;;;sBP8BkCnB,C,GADvCT,YAGIU,CAAAA,CAAAA,IAHJV,GAGIU,CAAAA,CAAAA,CAAAA,CAAAA,KAAAA,CAAAA,CAAAA,CAHJV,GAGIU,aAAAA,CAAAA,CAAAA,IAAAA,IAAAA,aAAAA,CAAAA,CAAAA,IAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,UAAAA,CAAAA,CAAAA,CAAAA,GAAAA,CAAAA,KAAAA,C,GAAAA,CAAAA,C;;AOhCCkB,O;;UPiDmBjB,CAAAA,CAAAA,K,EAAAA;UACFC,K,GAAAA,C;;gCAOOC,M,EAAAA,CAAAA,E,EAAAA;;aA8DvBC,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,C,MAAAA,CAAAA,CA8MJgB,CAAAA,CAAAA,IA9MIhB,CAAAA,GA8MJgB,C;;;;;;;;;;;qBMtBkB2B,M,GAAqB7B,C;;;;;;;;;6BAqFnCuC,C,IAAAA,O,EACA4B,CAAAA,CAAgB5B,CAAhB4B,CAAAA,GAAgB5B,U,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,c;;;;;;kBAWhB6B,C;cACKzC,CAAAA,E;;;;YAIQE,IAAAA,GAAAA;;;;;;qBAabwC,IAAAA,GAAAA,E;;;;;qBAWoBC,iBAAAA,OAAAA,CAOlBC,GAPkBD,CAOlBC,Q,EAAAA;;;;;;;;;;;;;;;;cAoBCC,CAAAA,CAAAA,Q,KAAAA,E;;;kMAWH,E;;;;;AAkBAjC,QAAAA,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAgBkC,CAAAA,CAAAA,YAAhBlC,EAAuDb,MAAvDa;;;aAOEV,C,IACKvE,C,EAAAA;2BAOPuE,OAAAA,CAAAA,GAAAA,CAAAA,Q,IAAAA,KAAAA,CAAAA,KAAAA,CAAAA,CAAAA,CAAAA,C,IAAAA,CAAAA,CAAAA,kCAAAA,CAAAA,GAAAA,0FAAAA,EAAAA,EAAAA,C;;;;;;;;;;;;wBEngBU+C,CAAAA,CAAAA,K,EAAAA,M;;AAUdA,iBAAAA,CAAAA,IAAAA,CAAAA,CAAAA,KAAAA,EAAAA;sBAGOC,EAAAA,CAAAA,WAAAA,CAAAA,C;;;;;eCoPoCkB,IAAAA,CAAAA,CAAShD,KAATgD,CAAShD,CAATgD,CAAAA,EAAShD;;;;;;;;;;SAATgD,M;AAhNfjB,UAAAA,EAAAA,CAAAA,CAAAA,CAAAA;;;;;aA0JoBd,M;;;yBAG5CwB,OAAAA,CAAAA,GAAAA,CAAAA,Q,IAAAA,CAAAA,CAAAA,Q,EAAAA;;;gBAO0BC,C,EAAAA;;;;;;;;;eA3JaT,C,EAAAA,C;;;;;YTihB3B5F,C,EAAAA,C,EAAAA,C,EAAAA,C,EAAAA;;;;;;;mBAWhBH,OAAAA,CAAAA,GAAAA,CAAAA,Q,IAAAA,CAAAA,CAAAA,GAAAA,E;;;;eSrSakH;;;;;;;;;;;;;;;;;SAwEkBR,CAAAA,E,GAAAA;;;;;;;;;;;;;eAsBdU,C,EAAoBC,C,EAAAA;;;;;;;;;;;;;SAtVzBxB,UAAAA,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,WAAAA,CAAAA,CAAAA,CAAAA,IAAAA,KAAAA,CAAAA,MACVC,CAAAA,GAAYD,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,EAAAA,CADFA,IACEA,IADFA,GACEA,CADFA,GACEA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,UAAAA,EAAAA,CAAAA,EAAAA,CAAAA,C;;;AA8LNc,SAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAwBC,CAAxBD,EAAwBC,CAAxBD,EAAwBC,CAAxBD,EAAwBC;AAAxBD,MAAAA,CAAAA;AAAAA,MAAAA,CAAAA;AAAAA,MAAAA,CAAAA;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CD0MdmD;AAAAA,MC1McnD,CAAAA,GD8MdqD,CAAAA,CAAAA,KAJAF;AAAAA,MAIAE,CAAAA,GAcqDD,CAAAA,KAApCG,CAAAA,CAAAA,UAAAA,CAAAA,KAlBjBJ;AAAAA,MAkBiBI,CAAAA,GAWrBH,CAAAA,IAAAA,CAAAA,CAAAA,WAAAA,CANAI,CAMAJ,CAAAA,IANAI,CAvBIL;mBAkCJK,OAAAA,CAAAA,GAAAA,CAAAA,Q,IAAAA,CAAAA,C,IAAAA,CAAAA,CAAAA,KAAAA,CAAAA,SAAAA,CAAAA,CAAAA,C,IAAAA,CAAAA,CAAAA,4DAAAA,CAAAA,GAAAA,0CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,UAAAA,CAAAA,QAAAA,GAAAA,SAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,UAAAA,CAAAA,YAAAA,GAAAA,kFAAAA,EAAAA,EAAAA,C;;kCAQIC,C,GASJ/G,CAAAA,CAAAA,CAAAA,EAAAA,YAAAA,CAAAA,IAAAA,CAAAA,IAAAA,CAAAA,CAAAA,U,GAAAA;aAyEAA,CAAAA,KAAAA,CAAAA,CAAAA,U,EAAAA;+BAcA8G,G,CAFIG,Q,IAAAA,CAAAA,CAAAA,6CAAAA,CAAAA,GAAAA,6EAAAA,EAAAA,CAAAA,C;;;;;AClVc3D,QAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,GAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,SAAAA,CAAAA,EAAAA,CAAAA,GAAAA,CAAAA,CAAAA,KAAAA,GAAAA,CAAAA,CAAAA,KAAAA,CAAAA,KAAAA,GAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,GAAAA,CAAAA,GAAAA,GAAAA,IAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAAA,E,WAAAA,E,qBAAAA,E,kBAAAA;;YA0BVmC,iBAAAA,OAAAA,CAAAA,GAAAA,CAAAA,QAAAA,IAAAA,CAAAA,CAAAA,MAAAA,IAAAA,C,EAAAA;;;;;YAcY,iBAAA,C,EAAA;iBAETtH,C;;;;;;;gBAUToJ,C;;;;;;AAWIC,YAAAA,CAAAA,CAAAA,YAAAA,GAAAA,KAAAA,CAAAA,KAAAA,CAAAA,GAAAA,CAAAA,GAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,GAAAA,KAAAA,CAAAA,MAAAA,CAAAA,GAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,IAAAA,CAAAA,GAAAA,EAAAA,CAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,EAAAA,CAAAA,CAAAA,CAAAA,CAAAA,CAAAA,GAAAA,YAAAA,OAAAA,CAAAA,IAAAA,SAAAA,CAAAA,KAAAA,CAAAA,GAAAA,CAAAA,CAAAA,GAAAA,CAAAA,GAAAA,CAAAA;;;0BAlF6BJ,EAAAA,CAAiB3B,CAAjB2B,C,KAAhBvE,CAAAA,GAAAA,CAAAA,CAAAA,EAAAA,CAAAA,GAAAA,I;;uBAIhBwE,I,CAAAA,G;;gCPNsCxD,M,IAAAA,EAAAA,CAAAA,CAAAA,CAAAA,MAAAA,EOWrCvE,CPXqCuE,EOWrCvE,CPXqCuE,C,EOWrCvE;;;;;;;;;;;;;;;;;;;qBA0FW+D,C,EAAGlF,C,EAAAA,C,EAAAA,C,EAAAA;;;;uBAShBsJ,M,IAAAA,EAAAA,CAAAA,CAAAA,CAAAA,MAAAA,EAAAA,CAAAA,EAAAA,CAAAA,C;;;;;;;;;qBAgBAX,C,IADAY,C,EAAAA;;;AA3FJJ;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;;SPzDyDlJ,K,QAAAA,8B;;;SAYxCA,kB,EAAAA,a,EAAAA,c,EAAAA,a,QAAAA,Y;KNRbM,IAAAA,GAAAA,E,EAAAA,CAAAA,GAAAA,E;;6CGoWEsC,C;;;;;;AS1HFuF,CAAAA,CAAAA,SAAAA,CACF5E,WADE4E,GACmCA,UAAAA,CAAAA,EAAAA;oBAGxBC,CAAAA,KAAAA,EAAAA,CAAAA,OAAAA,CAAAA,M,EAAAA;;;;;;;;;WA/EZV,I;;;;;;;;YA5CDG,CAAAA,CAAAA,U,IT4BJlG,CAAAA,GAAAA,KAAAA,IAAAA,CAAAA,CAAAA,CAAAA,UAAAA,EAAAA,CAAAA,C,GAAAA,QAAAA,CAAAA,CAAAA,EAAAA,GAAAA,CAAAA,GAAAA,KAAAA,CAAAA,CAAAA,EAAAA,GAAAA,QAAAA,CAAAA,CAAAA,GAAAA,KAAAA,CAAAA,GAAAA,KAAAA,CAAAA,CAAAA,GAAAA,C;;CS2FMwG;;ARpLuBrE,CAAAA,CAAAA,SAAAA,CAAAA,OAAAA,GAAAA,UAAAA,CAAAA,EAAAA,CAAAA,EAAAA,CAAAA,EAAAA;;;6BAUdH,C,IAAAA;;;;;CAVcG;;;;2BQwEL2C,C;wBACWrE,OAAAA,CAAAA,GAAAA,CAAAA,Q,GAAAA,YAAAA,yHAAAA,OAAAA,CAAAA,GAAAA,CAAAA,CAAAA,UAAAA,GAAAA,CAAAA,GAAAA,I,GAAAA,E,EAAAA,E;;mBAI7B2F,C;;;;;;;;;AAYKV,CAAAA,CAAAA,SAAAA,CAAAA,aAAAA,GAAAA,UAAAA,CAAAA,EAAAA;UACErB,CAAAA,GAAAA,KAAAA,WAAAA,CAAAA,CAAAA,C,IAAAA,EAAAA,CAAAA,CAAAA,C,GAAAA,E;CADFqB;;AAGDW,CAAAA,CAAAA,SAAAA,CAAAA,WAAAA,GAAAA,UAAAA,CAAAA,EAAAA,CAAAA,EAAAA;mEd3HDjI,CAAAA,CAAAA,K;iCACcC,C,OACrBC,EAAAA,CAAaC,IAAbD,EAAaC,CAAbD,EAAaC,CAAbD,C;CcyHQ+H;;;;;;;;;;8DApDEJ,C,IAAAA;UACFA,a,CAAAA,I,CAAAA,K;uBACFC,C,IAAAA,CAAAA,CAAAA,U,KAAAA,CAAAA,CAAAA,UAAAA,GAAAA,C;;;iCAM0BC,Q,IAAAA,CAAAA,CAAAA,gIAAAA,EAAAA,CAAAA,C;;;;;;;;;oCA3BZrB,C,IAAAA,CAAAA,CAAAA,GAAAA,MAAAA,CAGdiB,IAHcjB,CAGdiB,CAHcjB,CAAAA,EAGdiB,CAHcjB,C,GAKZkB","sourcesContent":["import {\n  FieldNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { Variables } from '../types';\n\n/** Evaluates a fields arguments taking vars into account */\nexport const getFieldArguments = (\n  node: FieldNode,\n  vars: Variables\n): null | Variables => {\n  const args = {};\n  let argsSize = 0;\n  if (node.arguments && node.arguments.length) {\n    for (let i = 0, l = node.arguments.length; i < l; i++) {\n      const arg = node.arguments[i];\n      const value = valueFromASTUntyped(arg.value, vars);\n      if (value !== undefined && value !== null) {\n        args[getName(arg)] = value;\n        argsSize++;\n      }\n    }\n  }\n\n  return argsSize > 0 ? args : null;\n};\n\n/** Returns a filtered form of variables with values missing that the query doesn't require */\nexport const filterVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n) => {\n  if (!input || !node.variableDefinitions) {\n    return undefined;\n  }\n\n  const vars = {};\n  for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n    const name = getName(node.variableDefinitions[i].variable);\n    vars[name] = input[name];\n  }\n\n  return vars;\n};\n\n/** Returns a normalized form of variables with defaulted values */\nexport const normalizeVariables = (\n  node: OperationDefinitionNode,\n  input: void | object\n): Variables => {\n  const vars = {};\n  if (!input) return vars;\n\n  if (node.variableDefinitions) {\n    for (let i = 0, l = node.variableDefinitions.length; i < l; i++) {\n      const def = node.variableDefinitions[i];\n      const name = getName(def.variable);\n      vars[name] =\n        input[name] === undefined && def.defaultValue\n          ? valueFromASTUntyped(def.defaultValue, input)\n          : input[name];\n    }\n  }\n\n  for (const key in input) {\n    if (!(key in vars)) vars[key] = input[key];\n  }\n\n  return vars;\n};\n","// These are guards that are used throughout the codebase to warn or error on\n// unexpected behaviour or conditions.\n// Every warning and error comes with a number that uniquely identifies them.\n// You can read more about the messages themselves in `docs/graphcache/errors.md`\n\nimport { Kind, ExecutableDefinitionNode, InlineFragmentNode } from 'graphql';\n\nexport type ErrorCode =\n  | 1\n  | 2\n  | 3\n  | 4\n  | 5\n  | 6\n  | 7\n  | 8\n  | 9\n  | 10\n  | 11\n  | 12\n  | 13\n  | 14\n  | 15\n  | 16\n  | 17\n  | 18\n  | 19\n  | 20\n  | 21\n  | 22\n  | 23\n  | 24\n  | 25\n  | 26;\n\ntype DebugNode = ExecutableDefinitionNode | InlineFragmentNode;\n\n// URL unfurls to https://formidable.com/open-source/urql/docs/graphcache/errors/\nconst helpUrl = '\\nhttps://bit.ly/2XbVrpR#';\nconst cache = new Set<string>();\n\nexport const currentDebugStack: string[] = [];\n\nexport const popDebugNode = () => currentDebugStack.pop();\n\nexport const pushDebugNode = (typename: void | string, node: DebugNode) => {\n  let identifier = '';\n  if (node.kind === Kind.INLINE_FRAGMENT) {\n    identifier = typename\n      ? `Inline Fragment on \"${typename}\"`\n      : 'Inline Fragment';\n  } else if (node.kind === Kind.OPERATION_DEFINITION) {\n    const name = node.name ? `\"${node.name.value}\"` : 'Unnamed';\n    identifier = `${name} ${node.operation}`;\n  } else if (node.kind === Kind.FRAGMENT_DEFINITION) {\n    identifier = `\"${node.name.value}\" Fragment`;\n  }\n\n  if (identifier) {\n    currentDebugStack.push(identifier);\n  }\n};\n\nconst getDebugOutput = (): string =>\n  currentDebugStack.length\n    ? '\\n(Caused At: ' + currentDebugStack.join(', ') + ')'\n    : '';\n\nexport function invariant(\n  condition: any,\n  message: string,\n  code: ErrorCode\n): asserts condition {\n  if (!condition) {\n    let errorMessage = message || 'Minfied Error #' + code + '\\n';\n    if (process.env.NODE_ENV !== 'production') {\n      errorMessage += getDebugOutput();\n    }\n\n    const error = new Error(errorMessage + helpUrl + code);\n    error.name = 'Graphcache Error';\n    throw error;\n  }\n}\n\nexport function warn(message: string, code: ErrorCode) {\n  if (!cache.has(message)) {\n    console.warn(message + getDebugOutput() + helpUrl + code);\n    cache.add(message);\n  }\n}\n","import {\n  SelectionNode,\n  DocumentNode,\n  OperationDefinitionNode,\n  valueFromASTUntyped,\n  Kind,\n} from 'graphql';\n\nimport { getName } from './node';\n\nimport { invariant } from '../helpers/help';\nimport { Fragments, Variables } from '../types';\n\n/** Returns the main operation's definition */\nexport const getMainOperation = (\n  doc: DocumentNode\n): OperationDefinitionNode => {\n  for (let i = 0; i < doc.definitions.length; i++) {\n    if (doc.definitions[i].kind === Kind.OPERATION_DEFINITION) {\n      return doc.definitions[i] as OperationDefinitionNode;\n    }\n  }\n\n  invariant(\n    false,\n    'Invalid GraphQL document: All GraphQL documents must contain an OperationDefinition' +\n      'node for a query, subscription, or mutation.',\n    1\n  );\n};\n\n/** Returns a mapping from fragment names to their selections */\nexport const getFragments = (doc: DocumentNode): Fragments => {\n  const fragments: Fragments = {};\n  for (let i = 0; i < doc.definitions.length; i++) {\n    const node = doc.definitions[i];\n    if (node.kind === Kind.FRAGMENT_DEFINITION) {\n      fragments[getName(node)] = node;\n    }\n  }\n\n  return fragments;\n};\n\nexport const shouldInclude = (\n  node: SelectionNode,\n  vars: Variables\n): boolean => {\n  const { directives } = node;\n  if (!directives) return true;\n\n  // Finds any @include or @skip directive that forces the node to be skipped\n  for (let i = 0, l = directives.length; i < l; i++) {\n    const directive = directives[i];\n    const name = getName(directive);\n\n    if (\n      (name === 'include' || name === 'skip') &&\n      directive.arguments &&\n      directive.arguments[0] &&\n      getName(directive.arguments[0]) === 'if'\n    ) {\n      // Return whether this directive forces us to skip\n      // `@include(if: false)` or `@skip(if: true)`\n      const value = valueFromASTUntyped(directive.arguments[0].value, vars);\n      return name === 'include' ? !!value : !value;\n    }\n  }\n\n  return true;\n};\n","import {\n  IntrospectionQuery,\n  IntrospectionSchema,\n  IntrospectionInputValue,\n  IntrospectionTypeRef,\n  IntrospectionType,\n} from 'graphql';\n\nexport interface SchemaField {\n  name: string;\n  type: IntrospectionTypeRef;\n  args: Record<string, IntrospectionInputValue>;\n}\n\nexport interface SchemaObject {\n  name: string;\n  kind: 'INTERFACE' | 'OBJECT';\n  interfaces: Record<string, unknown>;\n  fields: Record<string, SchemaField>;\n}\n\nexport interface SchemaUnion {\n  name: string;\n  kind: 'UNION';\n  types: Record<string, unknown>;\n}\n\nexport interface SchemaIntrospector {\n  query: string | null;\n  mutation: string | null;\n  subscription: string | null;\n  types?: Record<string, SchemaObject | SchemaUnion>;\n  isSubType(abstract: string, possible: string): boolean;\n}\n\nexport interface PartialIntrospectionSchema {\n  queryType: { name: string; kind?: any };\n  mutationType?: { name: string; kind?: any };\n  subscriptionType?: { name: string; kind?: any };\n  types?: IntrospectionSchema['types'];\n}\n\nexport type IntrospectionData =\n  | IntrospectionQuery\n  | { __schema: PartialIntrospectionSchema };\n\nexport const buildClientSchema = ({\n  __schema,\n}: IntrospectionData): SchemaIntrospector => {\n  const typemap: Record<string, SchemaObject | SchemaUnion> = {};\n\n  const buildNameMap = <T extends { name: string }>(\n    arr: ReadonlyArray<T>\n  ): { [name: string]: T } => {\n    const map: Record<string, T> = {};\n    for (let i = 0; i < arr.length; i++) map[arr[i].name] = arr[i];\n    return map;\n  };\n\n  const buildType = (\n    type: IntrospectionType\n  ): SchemaObject | SchemaUnion | void => {\n    switch (type.kind) {\n      case 'OBJECT':\n      case 'INTERFACE':\n        return {\n          name: type.name,\n          kind: type.kind as 'OBJECT' | 'INTERFACE',\n          interfaces: buildNameMap(type.interfaces || []),\n          fields: buildNameMap(\n            type.fields.map(field => ({\n              name: field.name,\n              type: field.type,\n              args: buildNameMap(field.args),\n            }))\n          ),\n        } as SchemaObject;\n      case 'UNION':\n        return {\n          name: type.name,\n          kind: type.kind as 'UNION',\n          types: buildNameMap(type.possibleTypes || []),\n        } as SchemaUnion;\n    }\n  };\n\n  const schema: SchemaIntrospector = {\n    query: __schema.queryType ? __schema.queryType.name : null,\n    mutation: __schema.mutationType ? __schema.mutationType.name : null,\n    subscription: __schema.subscriptionType\n      ? __schema.subscriptionType.name\n      : null,\n    types: undefined,\n    isSubType(abstract: string, possible: string) {\n      const abstractType = typemap[abstract];\n      const possibleType = typemap[possible];\n      if (!abstractType || !possibleType) {\n        return false;\n      } else if (abstractType.kind === 'UNION') {\n        return !!abstractType.types[possible];\n      } else if (\n        abstractType.kind !== 'OBJECT' &&\n        possibleType.kind === 'OBJECT'\n      ) {\n        return !!possibleType.interfaces[abstract];\n      } else {\n        return abstract === possible;\n      }\n    },\n  };\n\n  if (__schema.types) {\n    schema.types = typemap;\n    for (let i = 0; i < __schema.types.length; i++) {\n      const type = __schema.types[i];\n      if (type && type.name) {\n        const out = buildType(type);\n        if (out) typemap[type.name] = out;\n      }\n    }\n  }\n\n  return schema;\n};\n","import { InlineFragmentNode, FragmentDefinitionNode } from 'graphql';\n\nimport { warn, invariant } from '../helpers/help';\nimport { getTypeCondition } from './node';\nimport { SchemaIntrospector, SchemaObject } from './schema';\n\nimport {\n  KeyingConfig,\n  UpdateResolver,\n  ResolverConfig,\n  OptimisticMutationConfig,\n} from '../types';\n\nconst BUILTIN_FIELD_RE = /^__/;\n\nexport const isFieldNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  if (BUILTIN_FIELD_RE.test(fieldName)) return true;\n  const field = getField(schema, typename, fieldName);\n  return !!field && field.type.kind !== 'NON_NULL';\n};\n\nexport const isListNullable = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  const field = getField(schema, typename, fieldName);\n  if (!field) return false;\n  const ofType =\n    field.type.kind === 'NON_NULL' ? field.type.ofType : field.type;\n  return ofType.kind === 'LIST' && ofType.ofType.kind !== 'NON_NULL';\n};\n\nexport const isFieldAvailableOnType = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n): boolean => {\n  if (BUILTIN_FIELD_RE.test(fieldName)) return true;\n  return !!getField(schema, typename, fieldName);\n};\n\nexport const isInterfaceOfType = (\n  schema: SchemaIntrospector,\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: string | void\n): boolean => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) return true;\n  if (\n    schema.types![typeCondition] &&\n    schema.types![typeCondition].kind === 'OBJECT'\n  )\n    return typeCondition === typename;\n  expectAbstractType(schema, typeCondition!);\n  expectObjectType(schema, typename!);\n  return schema.isSubType(typeCondition, typename);\n};\n\nconst getField = (\n  schema: SchemaIntrospector,\n  typename: string,\n  fieldName: string\n) => {\n  expectObjectType(schema, typename);\n  const object = schema.types![typename] as SchemaObject;\n  const field = object.fields[fieldName];\n  if (!field) {\n    warn(\n      'Invalid field: The field `' +\n        fieldName +\n        '` does not exist on `' +\n        typename +\n        '`, ' +\n        'but the GraphQL document expects it to exist.\\n' +\n        'Traversal will continue, however this may lead to undefined behavior!',\n      4\n    );\n  }\n\n  return field;\n};\n\nfunction expectObjectType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types![typename] && schema.types![typename].kind === 'OBJECT',\n    'Invalid Object type: The type `' +\n      typename +\n      '` is not an object in the defined schema, ' +\n      'but the GraphQL document is traversing it.',\n    3\n  );\n}\n\nfunction expectAbstractType(schema: SchemaIntrospector, typename: string) {\n  invariant(\n    schema.types![typename] &&\n      (schema.types![typename].kind === 'INTERFACE' ||\n        schema.types![typename].kind === 'UNION'),\n    'Invalid Abstract type: The type `' +\n      typename +\n      '` is not an Interface or Union type in the defined schema, ' +\n      'but a fragment in the GraphQL document is using it as a type condition.',\n    5\n  );\n}\n\nexport function expectValidKeyingConfig(\n  schema: SchemaIntrospector,\n  keys: KeyingConfig\n): void {\n  if (process.env.NODE_ENV !== 'production') {\n    for (const key in keys) {\n      if (!schema.types![key]) {\n        warn(\n          'Invalid Object type: The type `' +\n            key +\n            '` is not an object in the defined schema, but the `keys` option is referencing it.',\n          20\n        );\n      }\n    }\n  }\n}\n\nexport function expectValidUpdatesConfig(\n  schema: SchemaIntrospector,\n  updates: Record<string, Record<string, UpdateResolver>>\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const mutationFields = (schema.types![schema.mutation] as SchemaObject)\n      .fields;\n    const givenMutations = updates[schema.mutation] || {};\n    for (const fieldName in givenMutations) {\n      if (mutationFields[fieldName] === undefined) {\n        warn(\n          'Invalid mutation field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Mutation` option is referencing it.',\n          21\n        );\n      }\n    }\n  }\n\n  if (schema.subscription) {\n    const subscriptionFields = (schema.types![\n      schema.subscription\n    ] as SchemaObject).fields;\n    const givenSubscription = updates[schema.subscription] || {};\n    for (const fieldName in givenSubscription) {\n      if (subscriptionFields[fieldName] === undefined) {\n        warn(\n          'Invalid subscription field: `' +\n            fieldName +\n            '` is not in the defined schema, but the `updates.Subscription` option is referencing it.',\n          22\n        );\n      }\n    }\n  }\n}\n\nfunction warnAboutResolver(name: string): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` is not in the defined schema, but the \\`resolvers\\` option is referencing it.`,\n    23\n  );\n}\n\nfunction warnAboutAbstractResolver(\n  name: string,\n  kind: 'UNION' | 'INTERFACE'\n): void {\n  warn(\n    `Invalid resolver: \\`${name}\\` does not match to a concrete type in the schema, but the \\`resolvers\\` option is referencing it. Implement the resolver for the types that ${\n      kind === 'UNION' ? 'make up the union' : 'implement the interface'\n    } instead.`,\n    26\n  );\n}\n\nexport function expectValidResolversConfig(\n  schema: SchemaIntrospector,\n  resolvers: ResolverConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  for (const key in resolvers) {\n    if (key === 'Query') {\n      if (schema.query) {\n        const validQueries = (schema.types![schema.query] as SchemaObject)\n          .fields;\n        for (const resolverQuery in resolvers.Query) {\n          if (!validQueries[resolverQuery]) {\n            warnAboutResolver('Query.' + resolverQuery);\n          }\n        }\n      } else {\n        warnAboutResolver('Query');\n      }\n    } else {\n      if (!schema.types![key]) {\n        warnAboutResolver(key);\n      } else if (\n        schema.types![key].kind === 'INTERFACE' ||\n        schema.types![key].kind === 'UNION'\n      ) {\n        warnAboutAbstractResolver(\n          key,\n          schema.types![key].kind as 'INTERFACE' | 'UNION'\n        );\n      } else {\n        const validTypeProperties = (schema.types![key] as SchemaObject).fields;\n        for (const resolverProperty in resolvers[key]) {\n          if (!validTypeProperties[resolverProperty]) {\n            warnAboutResolver(key + '.' + resolverProperty);\n          }\n        }\n      }\n    }\n  }\n}\n\nexport function expectValidOptimisticMutationsConfig(\n  schema: SchemaIntrospector,\n  optimisticMutations: OptimisticMutationConfig\n): void {\n  if (process.env.NODE_ENV === 'production') {\n    return;\n  }\n\n  if (schema.mutation) {\n    const validMutations = (schema.types![schema.mutation] as SchemaObject)\n      .fields;\n    for (const mutation in optimisticMutations) {\n      if (!validMutations[mutation]) {\n        warn(\n          `Invalid optimistic mutation field: \\`${mutation}\\` is not a mutation field in the defined schema, but the \\`optimistic\\` option is referencing it.`,\n          24\n        );\n      }\n    }\n  }\n}\n","import { stringifyVariables } from '@urql/core';\n\nimport {\n  Link,\n  EntityField,\n  FieldInfo,\n  StorageAdapter,\n  SerializedEntries,\n  Dependencies,\n  OperationType,\n} from '../types';\n\nimport {\n  serializeKeys,\n  deserializeKeyInfo,\n  fieldInfoOfKey,\n  joinKeys,\n} from './keys';\n\nimport { makeDict } from '../helpers/dict';\nimport { invariant, currentDebugStack } from '../helpers/help';\n\ntype Dict<T> = Record<string, T>;\ntype KeyMap<T> = Map<string, T>;\ntype OptimisticMap<T> = Record<number, T>;\n\ninterface NodeMap<T> {\n  optimistic: OptimisticMap<KeyMap<Dict<T | undefined>>>;\n  base: KeyMap<Dict<T>>;\n}\n\nexport interface InMemoryData {\n  /** Flag for whether deferred tasks have been scheduled yet */\n  defer: boolean;\n  /** A list of entities that have been flagged for gargabe collection since no references to them are left */\n  gc: Set<string>;\n  /** A list of entity+field keys that will be persisted */\n  persist: Set<string>;\n  /** The API's \"Query\" typename which is needed to filter dependencies */\n  queryRootKey: string;\n  /** Number of references to each entity (except \"Query\") */\n  refCount: Dict<number>;\n  /** Number of references to each entity on optimistic layers */\n  refLock: OptimisticMap<Dict<number>>;\n  /** A map of entity fields (key-value entries per entity) */\n  records: NodeMap<EntityField>;\n  /** A map of entity links which are connections from one entity to another (key-value entries per entity) */\n  links: NodeMap<Link>;\n  /** A set of Query operation keys that are in-flight and awaiting a result */\n  commutativeKeys: Set<number>;\n  /** The order of optimistic layers */\n  optimisticOrder: number[];\n  /** This may be a persistence adapter that will receive changes in a batch */\n  storage: StorageAdapter | null;\n}\n\nlet currentOperation: null | OperationType = null;\nlet currentData: null | InMemoryData = null;\nlet currentDependencies: null | Dependencies = null;\nlet currentOptimisticKey: null | number = null;\nlet currentOptimistic = false;\n\nconst makeNodeMap = <T>(): NodeMap<T> => ({\n  optimistic: makeDict(),\n  base: new Map(),\n});\n\n/** Before reading or writing the global state needs to be initialised */\nexport const initDataState = (\n  operationType: OperationType,\n  data: InMemoryData,\n  layerKey: number | null,\n  isOptimistic?: boolean\n) => {\n  currentOperation = operationType;\n  currentData = data;\n  currentDependencies = makeDict();\n  currentOptimistic = !!isOptimistic;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  if (!layerKey) {\n    currentOptimisticKey = null;\n  } else if (isOptimistic || data.optimisticOrder.length > 0) {\n    // If this operation isn't optimistic and we see it for the first time,\n    // then it must've been optimistic in the past, so we can proactively\n    // clear the optimistic data before writing\n    if (!isOptimistic && !data.commutativeKeys.has(layerKey)) {\n      reserveLayer(data, layerKey);\n    } else if (isOptimistic) {\n      // NOTE: This optimally shouldn't happen as it implies that an optimistic\n      // write is being performed after a concrete write.\n      data.commutativeKeys.delete(layerKey);\n    }\n\n    // An optimistic update of a mutation may force an optimistic layer,\n    // or this Query update may be applied optimistically since it's part\n    // of a commutative chain\n    currentOptimisticKey = layerKey;\n    createLayer(data, layerKey);\n  } else {\n    // Otherwise we don't create an optimistic layer and clear the\n    // operation's one if it already exists\n    currentOptimisticKey = null;\n    deleteLayer(data, layerKey);\n  }\n};\n\n/** Reset the data state after read/write is complete */\nexport const clearDataState = () => {\n  // NOTE: This is only called to check for the invariant to pass\n  if (process.env.NODE_ENV !== 'production') {\n    getCurrentDependencies();\n  }\n\n  const data = currentData!;\n  const layerKey = currentOptimisticKey;\n  currentOptimistic = false;\n  currentOptimisticKey = null;\n\n  // Determine whether the current operation has been a commutative layer\n  if (layerKey && data.optimisticOrder.indexOf(layerKey) > -1) {\n    // Squash all layers in reverse order (low priority upwards) that have\n    // been written already\n    let i = data.optimisticOrder.length;\n    while (\n      --i >= 0 &&\n      data.refLock[data.optimisticOrder[i]] &&\n      data.commutativeKeys.has(data.optimisticOrder[i])\n    ) {\n      squashLayer(data.optimisticOrder[i]);\n    }\n  }\n\n  currentOperation = null;\n  currentData = null;\n  currentDependencies = null;\n  if (process.env.NODE_ENV !== 'production') {\n    currentDebugStack.length = 0;\n  }\n\n  // Schedule deferred tasks if we haven't already\n  if (process.env.NODE_ENV !== 'test' && !data.defer) {\n    data.defer = true;\n    Promise.resolve().then(() => {\n      initDataState('read', data, null);\n      gc();\n      persistData();\n      clearDataState();\n      data.defer = false;\n    });\n  }\n};\n\n/** Initialises then resets the data state, which may squash this layer if necessary */\nexport const noopDataState = (\n  data: InMemoryData,\n  layerKey: number | null,\n  isOptimistic?: boolean\n) => {\n  initDataState('read', data, layerKey, isOptimistic);\n  clearDataState();\n};\n\nexport const getCurrentOperation = (): OperationType => {\n  invariant(\n    currentOperation !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentOperation;\n};\n\n/** As we're writing, we keep around all the records and links we've read or have written to */\nexport const getCurrentDependencies = (): Dependencies => {\n  invariant(\n    currentDependencies !== null,\n    'Invalid Cache call: The cache may only be accessed or mutated during' +\n      'operations like write or query, or as part of its resolvers, updaters, ' +\n      'or optimistic configs.',\n    2\n  );\n\n  return currentDependencies;\n};\n\nexport const make = (queryRootKey: string): InMemoryData => ({\n  defer: false,\n  gc: new Set(),\n  persist: new Set(),\n  queryRootKey,\n  refCount: makeDict(),\n  refLock: makeDict(),\n  links: makeNodeMap(),\n  records: makeNodeMap(),\n  commutativeKeys: new Set(),\n  optimisticOrder: [],\n  storage: null,\n});\n\n/** Adds a node value to a NodeMap (taking optimistic values into account */\nconst setNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string,\n  value: T\n) => {\n  // Optimistic values are written to a map in the optimistic dict\n  // All other values are written to the base map\n  const keymap: KeyMap<Dict<T | undefined>> = currentOptimisticKey\n    ? map.optimistic[currentOptimisticKey]\n    : map.base;\n\n  // On the map itself we get or create the entity as a dict\n  let entity = keymap.get(entityKey) as Dict<T | undefined>;\n  if (entity === undefined) {\n    keymap.set(entityKey, (entity = makeDict()));\n  }\n\n  // If we're setting undefined we delete the node's entry\n  // On optimistic layers we actually set undefined so it can\n  // override the base value\n  if (value === undefined && !currentOptimisticKey) {\n    delete entity[fieldKey];\n  } else {\n    entity[fieldKey] = value;\n  }\n};\n\n/** Gets a node value from a NodeMap (taking optimistic values into account */\nconst getNode = <T>(\n  map: NodeMap<T>,\n  entityKey: string,\n  fieldKey: string\n): T | undefined => {\n  let node: Dict<T | undefined> | undefined;\n  // A read may be initialised to skip layers until its own, which is useful for\n  // reading back written data. It won't skip over optimistic layers however\n  let skip =\n    !currentOptimistic &&\n    currentOperation === 'read' &&\n    currentOptimisticKey &&\n    currentData!.commutativeKeys.has(currentOptimisticKey);\n  // This first iterates over optimistic layers (in order)\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const layerKey = currentData!.optimisticOrder[i];\n    const optimistic = map.optimistic[layerKey];\n    // If we're reading starting from a specific layer, we skip until a match\n    skip = skip && layerKey !== currentOptimisticKey;\n    // If the node and node value exists it is returned, including undefined\n    if (\n      optimistic &&\n      (!skip || !currentData!.commutativeKeys.has(layerKey)) &&\n      (!currentOptimistic ||\n        currentOperation === 'write' ||\n        currentData!.commutativeKeys.has(layerKey)) &&\n      (node = optimistic.get(entityKey)) !== undefined &&\n      fieldKey in node\n    ) {\n      return node[fieldKey];\n    }\n  }\n\n  // Otherwise we read the non-optimistic base value\n  node = map.base.get(entityKey);\n  return node !== undefined ? node[fieldKey] : undefined;\n};\n\n/** Adjusts the reference count of an entity on a refCount dict by \"by\" and updates the gc */\nconst updateRCForEntity = (\n  gc: void | Set<string>,\n  refCount: Dict<number>,\n  entityKey: string,\n  by: number\n): void => {\n  // Retrieve the reference count\n  const count = refCount[entityKey] !== undefined ? refCount[entityKey] : 0;\n  // Adjust it by the \"by\" value\n  const newCount = (refCount[entityKey] = (count + by) | 0);\n  // Add it to the garbage collection batch if it needs to be deleted or remove it\n  // from the batch if it needs to be kept\n  if (gc !== undefined) {\n    if (newCount <= 0) gc.add(entityKey);\n    else if (count <= 0 && newCount > 0) gc.delete(entityKey);\n  }\n};\n\n/** Adjusts the reference counts of all entities of a link on a refCount dict by \"by\" and updates the gc */\nconst updateRCForLink = (\n  gc: void | Set<string>,\n  refCount: Dict<number>,\n  link: Link | undefined,\n  by: number\n): void => {\n  if (typeof link === 'string') {\n    updateRCForEntity(gc, refCount, link, by);\n  } else if (Array.isArray(link)) {\n    for (let i = 0, l = link.length; i < l; i++) {\n      const entityKey = link[i];\n      if (entityKey) {\n        updateRCForEntity(gc, refCount, entityKey, by);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of a given node dict to a given array if it hasn't been seen */\nconst extractNodeFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  node: Dict<T> | undefined\n): void => {\n  if (node !== undefined) {\n    for (const fieldKey in node) {\n      if (!seenFieldKeys.has(fieldKey)) {\n        // If the node hasn't been seen the serialized fieldKey is turnt back into\n        // a rich FieldInfo object that also contains the field's name and arguments\n        fieldInfos.push(fieldInfoOfKey(fieldKey));\n        seenFieldKeys.add(fieldKey);\n      }\n    }\n  }\n};\n\n/** Writes all parsed FieldInfo objects of all nodes in a NodeMap to a given array */\nconst extractNodeMapFields = <T>(\n  fieldInfos: FieldInfo[],\n  seenFieldKeys: Set<string>,\n  entityKey: string,\n  map: NodeMap<T>\n) => {\n  // Extracts FieldInfo for the entity in the base map\n  extractNodeFields(fieldInfos, seenFieldKeys, map.base.get(entityKey));\n\n  // Then extracts FieldInfo for the entity from the optimistic maps\n  for (let i = 0, l = currentData!.optimisticOrder.length; i < l; i++) {\n    const optimistic = map.optimistic[currentData!.optimisticOrder[i]];\n    if (optimistic !== undefined) {\n      extractNodeFields(fieldInfos, seenFieldKeys, optimistic.get(entityKey));\n    }\n  }\n};\n\n/** Garbage collects all entities that have been marked as having no references */\nexport const gc = () => {\n  // Iterate over all entities that have been marked for deletion\n  // Entities have been marked for deletion in `updateRCForEntity` if\n  // their reference count dropped to 0\n  currentData!.gc.forEach((entityKey: string, _, batch: Set<string>) => {\n    // Check first whether the reference count is still 0\n    const rc = currentData!.refCount[entityKey] || 0;\n    if (rc > 0) {\n      batch.delete(entityKey);\n      return;\n    }\n\n    // Each optimistic layer may also still contain some references to marked entities\n    for (const layerKey in currentData!.refLock) {\n      const refCount = currentData!.refLock[layerKey];\n      const locks = refCount[entityKey] || 0;\n      // If the optimistic layer has any references to the entity, don't GC it,\n      // otherwise delete the reference count from the optimistic layer\n      if (locks > 0) return;\n      delete refCount[entityKey];\n    }\n\n    // Delete the reference count, and delete the entity from the GC batch\n    delete currentData!.refCount[entityKey];\n    batch.delete(entityKey);\n    currentData!.records.base.delete(entityKey);\n    const linkNode = currentData!.links.base.get(entityKey);\n    if (linkNode) {\n      currentData!.links.base.delete(entityKey);\n      for (const fieldKey in linkNode) {\n        updateRCForLink(batch, currentData!.refCount, linkNode[fieldKey], -1);\n      }\n    }\n  });\n};\n\nconst updateDependencies = (entityKey: string, fieldKey?: string) => {\n  if (fieldKey !== '__typename') {\n    if (entityKey !== currentData!.queryRootKey) {\n      currentDependencies![entityKey] = true;\n    } else if (fieldKey !== undefined) {\n      currentDependencies![joinKeys(entityKey, fieldKey)] = true;\n    }\n  }\n};\n\nconst updatePersist = (entityKey: string, fieldKey: string) => {\n  if (!currentOptimistic && currentData!.storage) {\n    currentData!.persist.add(serializeKeys(entityKey, fieldKey));\n  }\n};\n\n/** Reads an entity's field (a \"record\") from data */\nexport const readRecord = (\n  entityKey: string,\n  fieldKey: string\n): EntityField => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.records, entityKey, fieldKey);\n};\n\n/** Reads an entity's link from data */\nexport const readLink = (\n  entityKey: string,\n  fieldKey: string\n): Link | undefined => {\n  updateDependencies(entityKey, fieldKey);\n  return getNode(currentData!.links, entityKey, fieldKey);\n};\n\n/** Writes an entity's field (a \"record\") to data */\nexport const writeRecord = (\n  entityKey: string,\n  fieldKey: string,\n  value?: EntityField\n) => {\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  setNode(currentData!.records, entityKey, fieldKey, value);\n};\n\nexport const hasField = (entityKey: string, fieldKey: string): boolean =>\n  readRecord(entityKey, fieldKey) !== undefined ||\n  readLink(entityKey, fieldKey) !== undefined;\n\n/** Writes an entity's link to data */\nexport const writeLink = (\n  entityKey: string,\n  fieldKey: string,\n  link?: Link | undefined\n) => {\n  const data = currentData!;\n  // Retrieve the reference counting dict or the optimistic reference locking dict\n  let refCount: Dict<number>;\n  // Retrive the link NodeMap from either an optimistic or the base layer\n  let links: KeyMap<Dict<Link | undefined>> | undefined;\n  // Set the GC batch if we're not optimistically updating\n  let gc: void | Set<string>;\n  if (currentOptimisticKey) {\n    // The refLock counters are also reference counters, but they prevent\n    // garbage collection instead of being used to trigger it\n    refCount =\n      data.refLock[currentOptimisticKey] ||\n      (data.refLock[currentOptimisticKey] = makeDict());\n    links = data.links.optimistic[currentOptimisticKey];\n  } else {\n    refCount = data.refCount;\n    links = data.links.base;\n    gc = data.gc;\n  }\n\n  // Retrieve the previous link for this field\n  const prevLinkNode = links && links.get(entityKey);\n  const prevLink = prevLinkNode && prevLinkNode[fieldKey];\n\n  // Update persistence batch and dependencies\n  updateDependencies(entityKey, fieldKey);\n  updatePersist(entityKey, fieldKey);\n  // Update the link\n  setNode(data.links, entityKey, fieldKey, link);\n  // First decrease the reference count for the previous link\n  updateRCForLink(gc, refCount, prevLink, -1);\n  // Then increase the reference count for the new link\n  updateRCForLink(gc, refCount, link, 1);\n};\n\n/** Reserves an optimistic layer and preorders it */\nexport const reserveLayer = (data: InMemoryData, layerKey: number) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index === -1) {\n    // The new layer needs to be reserved in front of all other commutative\n    // keys but after all non-commutative keys (which are added by `forceUpdate`)\n    data.optimisticOrder.unshift(layerKey);\n  } else if (!data.commutativeKeys.has(layerKey)) {\n    // Protect optimistic layers from being turned into non-optimistic layers\n    // while preserving optimistic data\n    clearLayer(data, layerKey);\n    // If the layer was an optimistic layer prior to this call, it'll be converted\n    // to a new non-optimistic layer and shifted ahead\n    data.optimisticOrder.splice(index, 1);\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  data.commutativeKeys.add(layerKey);\n};\n\n/** Creates an optimistic layer of links and records */\nconst createLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.optimisticOrder.indexOf(layerKey) === -1) {\n    data.optimisticOrder.unshift(layerKey);\n  }\n\n  if (!data.refLock[layerKey]) {\n    data.refLock[layerKey] = makeDict();\n    data.links.optimistic[layerKey] = new Map();\n    data.records.optimistic[layerKey] = new Map();\n  }\n};\n\n/** Clears all links and records of an optimistic layer */\nconst clearLayer = (data: InMemoryData, layerKey: number) => {\n  if (data.refLock[layerKey]) {\n    delete data.refLock[layerKey];\n    delete data.records.optimistic[layerKey];\n    delete data.links.optimistic[layerKey];\n  }\n};\n\n/** Deletes links and records of an optimistic layer, and the layer itself */\nconst deleteLayer = (data: InMemoryData, layerKey: number) => {\n  const index = data.optimisticOrder.indexOf(layerKey);\n  if (index > -1) {\n    data.optimisticOrder.splice(index, 1);\n    data.commutativeKeys.delete(layerKey);\n  }\n\n  clearLayer(data, layerKey);\n};\n\n/** Merges an optimistic layer of links and records into the base data */\nconst squashLayer = (layerKey: number) => {\n  // Hide current dependencies from squashing operations\n  const previousDependencies = currentDependencies;\n  currentDependencies = makeDict();\n\n  const links = currentData!.links.optimistic[layerKey];\n  if (links) {\n    links.forEach((keyMap, entityKey) => {\n      for (const fieldKey in keyMap)\n        writeLink(entityKey, fieldKey, keyMap[fieldKey]);\n    });\n  }\n\n  const records = currentData!.records.optimistic[layerKey];\n  if (records) {\n    records.forEach((keyMap, entityKey) => {\n      for (const fieldKey in keyMap)\n        writeRecord(entityKey, fieldKey, keyMap[fieldKey]);\n    });\n  }\n\n  currentDependencies = previousDependencies;\n  deleteLayer(currentData!, layerKey);\n};\n\n/** Return an array of FieldInfo (info on all the fields and their arguments) for a given entity */\nexport const inspectFields = (entityKey: string): FieldInfo[] => {\n  const { links, records } = currentData!;\n  const fieldInfos: FieldInfo[] = [];\n  const seenFieldKeys: Set<string> = new Set();\n  // Update dependencies\n  updateDependencies(entityKey);\n  // Extract FieldInfos to the fieldInfos array for links and records\n  // This also deduplicates by keeping track of fieldKeys in the seenFieldKeys Set\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, links);\n  extractNodeMapFields(fieldInfos, seenFieldKeys, entityKey, records);\n  return fieldInfos;\n};\n\nexport const persistData = () => {\n  if (currentData!.storage) {\n    currentOptimistic = true;\n    currentOperation = 'read';\n    const entries: SerializedEntries = makeDict();\n    currentData!.persist.forEach(key => {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      let x: void | Link | EntityField;\n      if ((x = readLink(entityKey, fieldKey)) !== undefined) {\n        entries[key] = `:${stringifyVariables(x)}`;\n      } else if ((x = readRecord(entityKey, fieldKey)) !== undefined) {\n        entries[key] = stringifyVariables(x);\n      } else {\n        entries[key] = undefined;\n      }\n    });\n\n    currentOptimistic = false;\n    currentData!.storage.writeData(entries);\n    currentData!.persist.clear();\n  }\n};\n\nexport const hydrateData = (\n  data: InMemoryData,\n  storage: StorageAdapter,\n  entries: SerializedEntries\n) => {\n  initDataState('write', data, null);\n\n  for (const key in entries) {\n    const value = entries[key];\n    if (value !== undefined) {\n      const { entityKey, fieldKey } = deserializeKeyInfo(key);\n      if (value[0] === ':') {\n        writeLink(entityKey, fieldKey, JSON.parse(value.slice(1)));\n      } else {\n        writeRecord(entityKey, fieldKey, JSON.parse(value));\n      }\n    }\n  }\n\n  clearDataState();\n  data.storage = storage;\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n  isFieldAvailableOnType,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n} from '../ast';\n\nimport { invariant, warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  NullArray,\n  Variables,\n  Data,\n  Link,\n  OperationRequest,\n  Dependencies,\n  EntityField,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n} from './shared';\n\nexport interface WriteResult {\n  data: null | Data;\n  dependencies: Dependencies;\n}\n\n/** Writes a request given its response to the store */\nexport const write = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  key?: number\n): WriteResult => {\n  initDataState('write', store.data, key || null);\n  const result = startWrite(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const writeOptimistic = (\n  store: Store,\n  request: OperationRequest,\n  key: number\n): WriteResult => {\n  if (process.env.NODE_ENV !== 'production') {\n    invariant(\n      getMainOperation(request.query).operation === 'mutation',\n      'writeOptimistic(...) was called with an operation that is not a mutation.\\n' +\n        'This case is unsupported and should never occur.',\n      10\n    );\n  }\n\n  initDataState('write', store.data, key, true);\n  const result = startWrite(store, request, {} as Data, undefined, true);\n  clearDataState();\n  return result;\n};\n\nexport const startWrite = (\n  store: Store,\n  request: OperationRequest,\n  data: Data,\n  error?: CombinedError | undefined,\n  isOptimistic?: boolean\n) => {\n  const operation = getMainOperation(request.query);\n  const result: WriteResult = { data, dependencies: getCurrentDependencies() };\n  const kind = store.rootFields[operation.operation];\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    kind,\n    kind,\n    !!isOptimistic,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(kind, operation);\n  }\n\n  writeSelection(ctx, kind, getSelectionSet(operation), data);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nexport const writeFragment = (\n  store: Store,\n  query: DocumentNode,\n  data: Partial<Data>,\n  variables?: Variables\n) => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (!fragment) {\n    return warn(\n      'writeFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      11\n    );\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  const dataToWrite = { __typename: typename, ...data } as Data;\n  const entityKey = store.keyOfEntity(dataToWrite);\n  if (!entityKey) {\n    return warn(\n      \"Can't generate a key for writeFragment(...) data.\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      12\n    );\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey,\n    undefined\n  );\n\n  writeSelection(ctx, entityKey, getSelectionSet(fragment), dataToWrite);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n};\n\nconst writeSelection = (\n  ctx: Context,\n  entityKey: undefined | string,\n  select: SelectionSet,\n  data: Data\n) => {\n  const isQuery = entityKey === ctx.store.rootFields['query'];\n  const isRoot = !isQuery && !!ctx.store.rootNames[entityKey!];\n  const typename = isRoot || isQuery ? entityKey : data.__typename;\n  if (!typename) {\n    warn(\n      \"Couldn't find __typename when writing.\\n\" +\n        \"If you're writing to the cache manually have to pass a `__typename` property on each entity in your data.\",\n      14\n    );\n    return;\n  } else if (!isRoot && !isQuery && entityKey) {\n    InMemoryData.writeRecord(entityKey, '__typename', typename);\n  }\n\n  const iterate = makeSelectionIterator(\n    typename,\n    entityKey || typename,\n    select,\n    ctx\n  );\n\n  let node: FieldNode | void;\n  while ((node = iterate())) {\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const fieldAlias = getFieldAlias(node);\n    let fieldValue = data[fieldAlias];\n\n    if (process.env.NODE_ENV !== 'production') {\n      if (!isRoot && fieldValue === undefined) {\n        const advice = ctx.optimistic\n          ? '\\nYour optimistic result may be missing a field!'\n          : '';\n\n        const expected =\n          node.selectionSet === undefined\n            ? 'scalar (number, boolean, etc)'\n            : 'selection set';\n\n        warn(\n          'Invalid undefined: The field at `' +\n            fieldKey +\n            '` is `undefined`, but the GraphQL query expects a ' +\n            expected +\n            ' for this field.' +\n            advice,\n          13\n        );\n\n        continue; // Skip this field\n      } else if (ctx.store.schema && typename && fieldName !== '__typename') {\n        isFieldAvailableOnType(ctx.store.schema, typename, fieldName);\n      }\n    }\n\n    // We simply skip all typenames fields and assume they've already been written above\n    if (fieldName === '__typename') continue;\n\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n\n    // Execute optimistic mutation functions on root fields\n    if (ctx.optimistic && isRoot) {\n      const resolver = ctx.store.optimisticMutations[fieldName];\n\n      if (!resolver) continue;\n      // We have to update the context to reflect up-to-date ResolveInfo\n      updateContext(ctx, data, typename, typename, fieldKey, fieldName);\n      fieldValue = data[fieldAlias] = ensureData(\n        resolver(fieldArgs || {}, ctx.store, ctx)\n      );\n    }\n\n    if (node.selectionSet) {\n      // Process the field and write links for the child entities that have been written\n      if (entityKey && !isRoot) {\n        const key = joinKeys(entityKey, fieldKey);\n        const link = writeField(\n          ctx,\n          getSelectionSet(node),\n          ensureData(fieldValue),\n          key\n        );\n        InMemoryData.writeLink(entityKey || typename, fieldKey, link);\n      } else {\n        writeField(ctx, getSelectionSet(node), ensureData(fieldValue));\n      }\n    } else if (entityKey && !isRoot) {\n      // This is a leaf node, so we're setting the field's value directly\n      InMemoryData.writeRecord(\n        entityKey || typename,\n        fieldKey,\n        (fieldValue !== null || !getFieldError(ctx)\n          ? fieldValue\n          : undefined) as EntityField\n      );\n    }\n\n    if (isRoot) {\n      // We run side-effect updates after the default, normalized updates\n      // so that the data is already available in-store if necessary\n      const updater = ctx.store.updates[typename][fieldName];\n      if (updater) {\n        // We have to update the context to reflect up-to-date ResolveInfo\n        updateContext(\n          ctx,\n          data,\n          typename,\n          typename,\n          joinKeys(typename, fieldKey),\n          fieldName\n        );\n\n        data[fieldName] = fieldValue;\n        updater(data, fieldArgs || {}, ctx.store, ctx);\n      }\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n};\n\n// A pattern to match typenames of types that are likely never keyable\nconst KEYLESS_TYPE_RE = /^__|PageInfo|(Connection|Edge)$/;\n\nconst writeField = (\n  ctx: Context,\n  select: SelectionSet,\n  data: null | Data | NullArray<Data>,\n  parentFieldKey?: string\n): Link | undefined => {\n  if (Array.isArray(data)) {\n    const newData = new Array(data.length);\n    for (let i = 0, l = data.length; i < l; i++) {\n      // Add the current index to the walked path before processing the link\n      ctx.__internal.path.push(i);\n      // Append the current index to the parentFieldKey fallback\n      const indexKey = parentFieldKey\n        ? joinKeys(parentFieldKey, `${i}`)\n        : undefined;\n      // Recursively write array data\n      const links = writeField(ctx, select, data[i], indexKey);\n      // Link cannot be expressed as a recursive type\n      newData[i] = links as string | null;\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return newData;\n  } else if (data === null) {\n    return getFieldError(ctx) ? undefined : null;\n  }\n\n  const entityKey = ctx.store.keyOfEntity(data);\n  const typename = data.__typename;\n\n  if (\n    parentFieldKey &&\n    !ctx.store.keys[data.__typename] &&\n    entityKey === null &&\n    typeof typename === 'string' &&\n    !KEYLESS_TYPE_RE.test(typename)\n  ) {\n    warn(\n      'Invalid key: The GraphQL query at the field at `' +\n        parentFieldKey +\n        '` has a selection set, ' +\n        'but no key could be generated for the data at this field.\\n' +\n        'You have to request `id` or `_id` fields for all selection sets or create ' +\n        'a custom `keys` config for `' +\n        typename +\n        '`.\\n' +\n        'Entities without keys will be embedded directly on the parent entity. ' +\n        'If this is intentional, create a `keys` config for `' +\n        typename +\n        '` that always returns null.',\n      15\n    );\n  }\n\n  const childKey = entityKey || parentFieldKey;\n  writeSelection(ctx, childKey, select, data);\n  return childKey || null;\n};\n","import { CombinedError } from '@urql/core';\nimport {\n  GraphQLError,\n  FieldNode,\n  InlineFragmentNode,\n  FragmentDefinitionNode,\n} from 'graphql';\n\nimport {\n  isInlineFragment,\n  getTypeCondition,\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  isFieldNode,\n} from '../ast';\n\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\nimport { hasField } from '../store/data';\nimport { Store, keyOfField } from '../store';\nimport { Fragments, Variables, DataField, NullArray, Data } from '../types';\nimport { getFieldArguments, shouldInclude, isInterfaceOfType } from '../ast';\n\nexport interface Context {\n  store: Store;\n  variables: Variables;\n  fragments: Fragments;\n  parentTypeName: string;\n  parentKey: string;\n  parentFieldKey: string;\n  parent: Data;\n  fieldName: string;\n  error: GraphQLError | undefined;\n  partial: boolean;\n  optimistic: boolean;\n  __internal: {\n    path: Array<string | number>;\n    errorMap: { [path: string]: GraphQLError } | undefined;\n  };\n}\n\nexport const contextRef: { current: Context | null } = { current: null };\n\n// Checks whether the current data field is a cache miss because of a GraphQLError\nexport const getFieldError = (ctx: Context): GraphQLError | undefined =>\n  ctx.__internal.path.length > 0 && ctx.__internal.errorMap\n    ? ctx.__internal.errorMap[ctx.__internal.path.join('.')]\n    : undefined;\n\nexport const makeContext = (\n  store: Store,\n  variables: Variables,\n  fragments: Fragments,\n  typename: string,\n  entityKey: string,\n  optimistic?: boolean,\n  error?: CombinedError | undefined\n): Context => {\n  const ctx: Context = {\n    store,\n    variables,\n    fragments,\n    parent: { __typename: typename },\n    parentTypeName: typename,\n    parentKey: entityKey,\n    parentFieldKey: '',\n    fieldName: '',\n    error: undefined,\n    partial: false,\n    optimistic: !!optimistic,\n    __internal: {\n      path: [],\n      errorMap: undefined,\n    },\n  };\n\n  if (error && error.graphQLErrors) {\n    for (let i = 0; i < error.graphQLErrors.length; i++) {\n      const graphQLError = error.graphQLErrors[i];\n      if (graphQLError.path && graphQLError.path.length) {\n        if (!ctx.__internal.errorMap)\n          ctx.__internal.errorMap = Object.create(null);\n        ctx.__internal.errorMap![graphQLError.path.join('.')] = graphQLError;\n      }\n    }\n  }\n\n  return ctx;\n};\n\nexport const updateContext = (\n  ctx: Context,\n  data: Data,\n  typename: string,\n  entityKey: string,\n  fieldKey: string,\n  fieldName: string\n) => {\n  contextRef.current = ctx;\n  ctx.parent = data;\n  ctx.parentTypeName = typename;\n  ctx.parentKey = entityKey;\n  ctx.parentFieldKey = fieldKey;\n  ctx.fieldName = fieldName;\n  ctx.error = getFieldError(ctx);\n};\n\nconst isFragmentHeuristicallyMatching = (\n  node: InlineFragmentNode | FragmentDefinitionNode,\n  typename: void | string,\n  entityKey: string,\n  vars: Variables\n) => {\n  if (!typename) return false;\n  const typeCondition = getTypeCondition(node);\n  if (!typeCondition || typename === typeCondition) return true;\n\n  warn(\n    'Heuristic Fragment Matching: A fragment is trying to match against the `' +\n      typename +\n      '` type, ' +\n      'but the type condition is `' +\n      typeCondition +\n      '`. Since GraphQL allows for interfaces `' +\n      typeCondition +\n      '` may be an' +\n      'interface.\\nA schema needs to be defined for this match to be deterministic, ' +\n      'otherwise the fragment will be matched heuristically!',\n    16\n  );\n\n  return !getSelectionSet(node).some(node => {\n    if (!isFieldNode(node)) return false;\n    const fieldKey = keyOfField(getName(node), getFieldArguments(node, vars));\n    return !hasField(entityKey, fieldKey);\n  });\n};\n\ninterface SelectionIterator {\n  (): FieldNode | undefined;\n}\n\nexport const makeSelectionIterator = (\n  typename: void | string,\n  entityKey: string,\n  select: SelectionSet,\n  ctx: Context\n): SelectionIterator => {\n  let childIterator: SelectionIterator | void;\n  let index = 0;\n\n  return function next() {\n    if (childIterator !== undefined) {\n      const node = childIterator();\n      if (node !== undefined) {\n        return node;\n      }\n\n      childIterator = undefined;\n      if (process.env.NODE_ENV !== 'production') {\n        popDebugNode();\n      }\n    }\n\n    while (index < select.length) {\n      const node = select[index++];\n      if (!shouldInclude(node, ctx.variables)) {\n        continue;\n      } else if (!isFieldNode(node)) {\n        // A fragment is either referred to by FragmentSpread or inline\n        const fragmentNode = !isInlineFragment(node)\n          ? ctx.fragments[getName(node)]\n          : node;\n\n        if (fragmentNode !== undefined) {\n          const isMatching = ctx.store.schema\n            ? isInterfaceOfType(ctx.store.schema, fragmentNode, typename)\n            : isFragmentHeuristicallyMatching(\n                fragmentNode,\n                typename,\n                entityKey,\n                ctx.variables\n              );\n\n          if (isMatching) {\n            if (process.env.NODE_ENV !== 'production') {\n              pushDebugNode(typename, fragmentNode);\n            }\n\n            return (childIterator = makeSelectionIterator(\n              typename,\n              entityKey,\n              getSelectionSet(fragmentNode),\n              ctx\n            ))();\n          }\n        }\n      } else {\n        return node;\n      }\n    }\n  };\n};\n\nexport const ensureData = (x: DataField): Data | NullArray<Data> | null =>\n  x === undefined ? null : (x as Data | NullArray<Data>);\n","import {\n  NamedTypeNode,\n  NameNode,\n  SelectionNode,\n  SelectionSetNode,\n  InlineFragmentNode,\n  FieldNode,\n  FragmentDefinitionNode,\n  Kind,\n} from 'graphql';\n\nexport type SelectionSet = ReadonlyArray<SelectionNode>;\n\n/** Returns the name of a given node */\nexport const getName = (node: { name: NameNode }): string => node.name.value;\n\nexport const getFragmentTypeName = (node: FragmentDefinitionNode): string =>\n  node.typeCondition.name.value;\n\n/** Returns either the field's name or the field's alias */\nexport const getFieldAlias = (node: FieldNode): string =>\n  node.alias ? node.alias.value : getName(node);\n\n/** Returns the SelectionSet for a given inline or defined fragment node */\nexport const getSelectionSet = (node: {\n  selectionSet?: SelectionSetNode;\n}): SelectionSet => (node.selectionSet ? node.selectionSet.selections : []);\n\nexport const getTypeCondition = (node: {\n  typeCondition?: NamedTypeNode;\n}): string | null => (node.typeCondition ? getName(node.typeCondition) : null);\n\nexport const isFieldNode = (node: SelectionNode): node is FieldNode =>\n  node.kind === Kind.FIELD;\n\nexport const isInlineFragment = (\n  node: SelectionNode\n): node is InlineFragmentNode => node.kind === Kind.INLINE_FRAGMENT;\n","import { DocumentNode } from 'graphql';\nimport { TypedDocumentNode, formatDocument, createRequest } from '@urql/core';\n\nimport {\n  Cache,\n  FieldInfo,\n  ResolverConfig,\n  DataField,\n  Variables,\n  Data,\n  QueryInput,\n  UpdatesConfig,\n  UpdateResolver,\n  OptimisticMutationConfig,\n  KeyingConfig,\n} from '../types';\n\nimport { invariant } from '../helpers/help';\nimport { contextRef } from '../operations/shared';\nimport { read, readFragment } from '../operations/query';\nimport { writeFragment, startWrite } from '../operations/write';\nimport { invalidateEntity } from '../operations/invalidate';\nimport { keyOfField } from './keys';\nimport * as InMemoryData from './data';\n\nimport {\n  IntrospectionData,\n  SchemaIntrospector,\n  buildClientSchema,\n  expectValidKeyingConfig,\n  expectValidUpdatesConfig,\n  expectValidResolversConfig,\n  expectValidOptimisticMutationsConfig,\n} from '../ast';\n\ntype RootField = 'query' | 'mutation' | 'subscription';\n\nexport interface StoreOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionData;\n}\n\nexport class Store implements Cache {\n  data: InMemoryData.InMemoryData;\n\n  resolvers: ResolverConfig;\n  updates: Record<string, Record<string, UpdateResolver>>;\n  optimisticMutations: OptimisticMutationConfig;\n  keys: KeyingConfig;\n  schema?: SchemaIntrospector;\n\n  rootFields: { query: string; mutation: string; subscription: string };\n  rootNames: { [name: string]: RootField };\n\n  constructor(opts?: StoreOpts) {\n    if (!opts) opts = {};\n\n    this.resolvers = opts.resolvers || {};\n    this.optimisticMutations = opts.optimistic || {};\n    this.keys = opts.keys || {};\n\n    let queryName = 'Query';\n    let mutationName = 'Mutation';\n    let subscriptionName = 'Subscription';\n    if (opts.schema) {\n      const schema = buildClientSchema(opts.schema);\n      queryName = schema.query || queryName;\n      mutationName = schema.mutation || mutationName;\n      subscriptionName = schema.subscription || subscriptionName;\n      // Only add schema introspector if it has types info\n      if (schema.types) this.schema = schema;\n    }\n\n    this.updates = {\n      [mutationName]: (opts.updates && opts.updates.Mutation) || {},\n      [subscriptionName]: (opts.updates && opts.updates.Subscription) || {},\n    };\n\n    this.rootFields = {\n      query: queryName,\n      mutation: mutationName,\n      subscription: subscriptionName,\n    };\n\n    this.rootNames = {\n      [queryName]: 'query',\n      [mutationName]: 'mutation',\n      [subscriptionName]: 'subscription',\n    };\n\n    this.data = InMemoryData.make(queryName);\n\n    if (this.schema && process.env.NODE_ENV !== 'production') {\n      expectValidKeyingConfig(this.schema, this.keys);\n      expectValidUpdatesConfig(this.schema, this.updates);\n      expectValidResolversConfig(this.schema, this.resolvers);\n      expectValidOptimisticMutationsConfig(\n        this.schema,\n        this.optimisticMutations\n      );\n    }\n  }\n\n  keyOfField = keyOfField;\n\n  keyOfEntity(data: Data | null | string) {\n    // In resolvers and updaters we may have a specific parent\n    // object available that can be used to skip to a specific parent\n    // key directly without looking at its incomplete properties\n    if (contextRef.current && data === contextRef.current.parent)\n      return contextRef.current!.parentKey;\n\n    if (data == null || typeof data === 'string') return data || null;\n    if (!data.__typename) return null;\n    if (this.rootNames[data.__typename]) return data.__typename;\n\n    let key: string | null | void;\n    if (this.keys[data.__typename]) {\n      key = this.keys[data.__typename](data);\n    } else if (data.id != null) {\n      key = `${data.id}`;\n    } else if (data._id != null) {\n      key = `${data._id}`;\n    }\n\n    return key ? `${data.__typename}:${key}` : null;\n  }\n\n  resolve(\n    entity: Data | string | null,\n    field: string,\n    args?: Variables\n  ): DataField {\n    const fieldKey = keyOfField(field, args);\n    const entityKey = this.keyOfEntity(entity);\n    if (!entityKey) return null;\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    if (fieldValue !== undefined) return fieldValue;\n    const link = InMemoryData.readLink(entityKey, fieldKey);\n    return link || null;\n  }\n\n  resolveFieldByKey = this.resolve;\n\n  invalidate(\n    entity: Data | string | null,\n    field?: string,\n    args?: Variables | null\n  ) {\n    const entityKey = this.keyOfEntity(entity);\n\n    invariant(\n      entityKey,\n      \"Can't generate a key for invalidate(...).\\n\" +\n        'You have to pass an id or _id field or create a custom `keys` field for `' +\n        typeof entity ===\n        'object'\n        ? (entity as Data).__typename\n        : entity + '`.',\n      19\n    );\n\n    invalidateEntity(entityKey, field, args);\n  }\n\n  inspectFields(entity: Data | string | null): FieldInfo[] {\n    const entityKey = this.keyOfEntity(entity);\n    return entityKey ? InMemoryData.inspectFields(entityKey) : [];\n  }\n\n  updateQuery<T = Data, V = Variables>(\n    input: QueryInput<T, V>,\n    updater: (data: T | null) => T | null\n  ): void {\n    const request = createRequest<T, V>(input.query, input.variables as any);\n    request.query = formatDocument(request.query);\n    const output = updater(this.readQuery(request));\n    if (output !== null) {\n      startWrite(this, request, output as any);\n    }\n  }\n\n  readQuery<T = Data, V = Variables>(input: QueryInput<T, V>): T | null {\n    const request = createRequest(input.query, input.variables!);\n    request.query = formatDocument(request.query);\n    return read(this, request).data as T | null;\n  }\n\n  readFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    entity: string | Data | T,\n    variables?: V\n  ): T | null {\n    return readFragment(\n      this,\n      formatDocument(fragment),\n      entity,\n      variables as any\n    ) as T | null;\n  }\n\n  writeFragment<T = Data, V = Variables>(\n    fragment: DocumentNode | TypedDocumentNode<T, V>,\n    data: T,\n    variables?: V\n  ): void {\n    writeFragment(this, formatDocument(fragment), data, variables as any);\n  }\n}\n","import * as InMemoryData from '../store/data';\nimport { Variables } from '../types';\nimport { keyOfField } from '../store';\n\ninterface PartialFieldInfo {\n  fieldKey: string;\n}\n\nexport const invalidateEntity = (\n  entityKey: string,\n  field?: string,\n  args?: Variables | null\n) => {\n  const fields: PartialFieldInfo[] = field\n    ? [{ fieldKey: keyOfField(field, args) }]\n    : InMemoryData.inspectFields(entityKey);\n\n  for (let i = 0, l = fields.length; i < l; i++) {\n    const { fieldKey } = fields[i];\n    if (InMemoryData.readLink(entityKey, fieldKey) !== undefined) {\n      InMemoryData.writeLink(entityKey, fieldKey, undefined);\n    } else {\n      InMemoryData.writeRecord(entityKey, fieldKey, undefined);\n    }\n  }\n};\n","import { FieldNode, DocumentNode, FragmentDefinitionNode } from 'graphql';\nimport { CombinedError } from '@urql/core';\n\nimport {\n  getSelectionSet,\n  getName,\n  SelectionSet,\n  getFragmentTypeName,\n  getFieldAlias,\n  getFragments,\n  getMainOperation,\n  normalizeVariables,\n  getFieldArguments,\n} from '../ast';\n\nimport {\n  Variables,\n  Data,\n  DataField,\n  Link,\n  OperationRequest,\n  NullArray,\n  Dependencies,\n} from '../types';\n\nimport {\n  Store,\n  getCurrentOperation,\n  getCurrentDependencies,\n  initDataState,\n  clearDataState,\n  joinKeys,\n  keyOfField,\n} from '../store';\n\nimport * as InMemoryData from '../store/data';\nimport { warn, pushDebugNode, popDebugNode } from '../helpers/help';\n\nimport {\n  Context,\n  makeSelectionIterator,\n  ensureData,\n  makeContext,\n  updateContext,\n  getFieldError,\n} from './shared';\n\nimport {\n  isFieldAvailableOnType,\n  isFieldNullable,\n  isListNullable,\n} from '../ast';\n\nexport interface QueryResult {\n  dependencies: Dependencies;\n  partial: boolean;\n  data: null | Data;\n}\n\nexport const query = (\n  store: Store,\n  request: OperationRequest,\n  data?: Data,\n  error?: CombinedError | undefined,\n  key?: number\n): QueryResult => {\n  initDataState('read', store.data, (data && key) || null);\n  const result = read(store, request, data, error);\n  clearDataState();\n  return result;\n};\n\nexport const read = (\n  store: Store,\n  request: OperationRequest,\n  input?: Data,\n  error?: CombinedError | undefined\n): QueryResult => {\n  const operation = getMainOperation(request.query);\n  const rootKey = store.rootFields[operation.operation];\n  const rootSelect = getSelectionSet(operation);\n\n  const ctx = makeContext(\n    store,\n    normalizeVariables(operation, request.variables),\n    getFragments(request.query),\n    rootKey,\n    rootKey,\n    false,\n    error\n  );\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(rootKey, operation);\n  }\n\n  // NOTE: This may reuse \"previous result data\" as indicated by the\n  // `originalData` argument in readRoot(). This behaviour isn't used\n  // for readSelection() however, which always produces results from\n  // scratch\n  const data =\n    rootKey !== ctx.store.rootFields['query']\n      ? readRoot(ctx, rootKey, rootSelect, input || ({} as Data))\n      : readSelection(ctx, rootKey, rootSelect, {} as Data);\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return {\n    dependencies: getCurrentDependencies(),\n    partial: ctx.partial || !data,\n    data: data || null,\n  };\n};\n\nconst readRoot = (\n  ctx: Context,\n  entityKey: string,\n  select: SelectionSet,\n  originalData: Data\n): Data => {\n  const typename = ctx.store.rootNames[entityKey]\n    ? entityKey\n    : originalData.__typename;\n  if (typeof typename !== 'string') {\n    return originalData;\n  }\n\n  const iterate = makeSelectionIterator(entityKey, entityKey, select, ctx);\n  const data = { __typename: typename };\n\n  let node: FieldNode | void;\n  while ((node = iterate())) {\n    const fieldAlias = getFieldAlias(node);\n    const fieldValue = originalData[fieldAlias];\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n    // Process the root field's value\n    if (node.selectionSet && fieldValue !== null) {\n      const fieldData = ensureData(fieldValue);\n      data[fieldAlias] = readRootField(ctx, getSelectionSet(node), fieldData);\n    } else {\n      data[fieldAlias] = fieldValue;\n    }\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n  }\n\n  return data;\n};\n\nconst readRootField = (\n  ctx: Context,\n  select: SelectionSet,\n  originalData: null | Data | NullArray<Data>\n): Data | NullArray<Data> | null => {\n  if (Array.isArray(originalData)) {\n    const newData = new Array(originalData.length);\n    for (let i = 0, l = originalData.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the root field's value\n      newData[i] = readRootField(ctx, select, originalData[i]);\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n    }\n\n    return newData;\n  } else if (originalData === null) {\n    return null;\n  }\n\n  // Write entity to key that falls back to the given parentFieldKey\n  const entityKey = ctx.store.keyOfEntity(originalData);\n  if (entityKey !== null) {\n    // We assume that since this is used for result data this can never be undefined,\n    // since the result data has already been written to the cache\n    const fieldValue = readSelection(ctx, entityKey, select, {} as Data);\n    return fieldValue === undefined ? null : fieldValue;\n  } else {\n    return readRoot(ctx, originalData.__typename, select, originalData);\n  }\n};\n\nexport const readFragment = (\n  store: Store,\n  query: DocumentNode,\n  entity: Partial<Data> | string,\n  variables?: Variables\n): Data | null => {\n  const fragments = getFragments(query);\n  const names = Object.keys(fragments);\n  const fragment = fragments[names[0]] as FragmentDefinitionNode;\n  if (!fragment) {\n    warn(\n      'readFragment(...) was called with an empty fragment.\\n' +\n        'You have to call it with at least one fragment in your GraphQL document.',\n      6\n    );\n\n    return null;\n  }\n\n  const typename = getFragmentTypeName(fragment);\n  if (typeof entity !== 'string' && !entity.__typename)\n    entity.__typename = typename;\n  const entityKey = store.keyOfEntity(entity as Data);\n  if (!entityKey) {\n    warn(\n      \"Can't generate a key for readFragment(...).\\n\" +\n        'You have to pass an `id` or `_id` field or create a custom `keys` config for `' +\n        typename +\n        '`.',\n      7\n    );\n\n    return null;\n  }\n\n  if (process.env.NODE_ENV !== 'production') {\n    pushDebugNode(typename, fragment);\n  }\n\n  const ctx = makeContext(\n    store,\n    variables || {},\n    fragments,\n    typename,\n    entityKey\n  );\n\n  const result =\n    readSelection(ctx, entityKey, getSelectionSet(fragment), {} as Data) ||\n    null;\n\n  if (process.env.NODE_ENV !== 'production') {\n    popDebugNode();\n  }\n\n  return result;\n};\n\nconst readSelection = (\n  ctx: Context,\n  key: string,\n  select: SelectionSet,\n  data: Data,\n  result?: Data\n): Data | undefined => {\n  const { store } = ctx;\n  const isQuery = key === store.rootFields['query'];\n\n  const entityKey = (result && store.keyOfEntity(result)) || key;\n  if (!isQuery && !!ctx.store.rootNames[entityKey]) {\n    warn(\n      'Invalid root traversal: A selection was being read on `' +\n        entityKey +\n        '` which is an uncached root type.\\n' +\n        'The `' +\n        ctx.store.rootFields.mutation +\n        '` and `' +\n        ctx.store.rootFields.subscription +\n        '` types are special ' +\n        'Operation Root Types and cannot be read back from the cache.',\n      25\n    );\n  }\n\n  const typename = !isQuery\n    ? InMemoryData.readRecord(entityKey, '__typename') ||\n      (result && result.__typename)\n    : key;\n\n  if (typeof typename !== 'string') {\n    return;\n  } else if (result && typename !== result.__typename) {\n    warn(\n      'Invalid resolver data: The resolver at `' +\n        entityKey +\n        '` returned an ' +\n        'invalid typename that could not be reconciled with the cache.',\n      8\n    );\n\n    return;\n  }\n\n  const iterate = makeSelectionIterator(typename, entityKey, select, ctx);\n\n  let node: FieldNode | void;\n  let hasFields = false;\n  let hasPartials = false;\n  while ((node = iterate()) !== undefined) {\n    // Derive the needed data from our node.\n    const fieldName = getName(node);\n    const fieldArgs = getFieldArguments(node, ctx.variables);\n    const fieldAlias = getFieldAlias(node);\n    const fieldKey = keyOfField(fieldName, fieldArgs);\n    const key = joinKeys(entityKey, fieldKey);\n    const fieldValue = InMemoryData.readRecord(entityKey, fieldKey);\n    const resultValue = result ? result[fieldName] : undefined;\n    const resolvers = store.resolvers[typename];\n\n    if (process.env.NODE_ENV !== 'production' && store.schema && typename) {\n      isFieldAvailableOnType(store.schema, typename, fieldName);\n    }\n\n    // We directly assign typenames and skip the field afterwards\n    if (fieldName === '__typename') {\n      data[fieldAlias] = typename;\n      continue;\n    }\n\n    // We temporarily store the data field in here, but undefined\n    // means that the value is missing from the cache\n    let dataFieldValue: void | DataField;\n    // Add the current alias to the walked path before processing the field's value\n    ctx.__internal.path.push(fieldAlias);\n\n    if (resultValue !== undefined && node.selectionSet === undefined) {\n      // The field is a scalar and can be retrieved directly from the result\n      dataFieldValue = resultValue;\n    } else if (\n      getCurrentOperation() === 'read' &&\n      resolvers &&\n      typeof resolvers[fieldName] === 'function'\n    ) {\n      // We have to update the information in context to reflect the info\n      // that the resolver will receive\n      updateContext(ctx, data, typename, entityKey, key, fieldName);\n\n      // We have a resolver for this field.\n      // Prepare the actual fieldValue, so that the resolver can use it\n      if (fieldValue !== undefined) {\n        data[fieldAlias] = fieldValue;\n      }\n\n      dataFieldValue = resolvers[fieldName](\n        data,\n        fieldArgs || ({} as Data),\n        store,\n        ctx\n      );\n\n      if (node.selectionSet) {\n        // When it has a selection set we are resolving an entity with a\n        // subselection. This can either be a list or an object.\n        dataFieldValue = resolveResolverResult(\n          ctx,\n          typename,\n          fieldName,\n          key,\n          getSelectionSet(node),\n          data[fieldAlias] as Data,\n          dataFieldValue\n        );\n      }\n\n      if (\n        store.schema &&\n        dataFieldValue === null &&\n        !isFieldNullable(store.schema, typename, fieldName)\n      ) {\n        // Special case for when null is not a valid value for the\n        // current field\n        return undefined;\n      }\n    } else if (!node.selectionSet) {\n      // The field is a scalar but isn't on the result, so it's retrieved from the cache\n      dataFieldValue = fieldValue;\n    } else if (resultValue !== undefined) {\n      // We start walking the nested resolver result here\n      dataFieldValue = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        key,\n        getSelectionSet(node),\n        data[fieldAlias] as Data,\n        resultValue\n      );\n    } else {\n      // Otherwise we attempt to get the missing field from the cache\n      const link = InMemoryData.readLink(entityKey, fieldKey);\n\n      if (link !== undefined) {\n        dataFieldValue = resolveLink(\n          ctx,\n          link,\n          typename,\n          fieldName,\n          getSelectionSet(node),\n          data[fieldAlias] as Data\n        );\n      } else if (typeof fieldValue === 'object' && fieldValue !== null) {\n        // The entity on the field was invalid but can still be recovered\n        dataFieldValue = fieldValue;\n      }\n    }\n\n    // If we have an error registered for the current field change undefined values to null\n    if (dataFieldValue === undefined && !!getFieldError(ctx)) {\n      hasPartials = true;\n      dataFieldValue = null;\n    }\n\n    // After processing the field, remove the current alias from the path again\n    ctx.__internal.path.pop();\n\n    // Now that dataFieldValue has been retrieved it'll be set on data\n    // If it's uncached (undefined) but nullable we can continue assembling\n    // a partial query result\n    if (\n      dataFieldValue === undefined &&\n      store.schema &&\n      isFieldNullable(store.schema, typename, fieldName)\n    ) {\n      // The field is uncached but we have a schema that says it's nullable\n      // Set the field to null and continue\n      hasPartials = true;\n      data[fieldAlias] = null;\n    } else if (dataFieldValue === undefined) {\n      // The field is uncached and not nullable; return undefined\n      return undefined;\n    } else {\n      // Otherwise continue as usual\n      hasFields = true;\n      data[fieldAlias] = dataFieldValue;\n    }\n  }\n\n  if (hasPartials) ctx.partial = true;\n  return isQuery && hasPartials && !hasFields ? undefined : data;\n};\n\nconst resolveResolverResult = (\n  ctx: Context,\n  typename: string,\n  fieldName: string,\n  key: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[],\n  result: void | DataField\n): DataField | void => {\n  if (Array.isArray(result)) {\n    const { store } = ctx;\n    // Check whether values of the list may be null; for resolvers we assume\n    // that they can be, since it's user-provided data\n    const _isListNullable =\n      !store.schema || isListNullable(store.schema, typename, fieldName);\n    const data = new Array(result.length);\n    for (let i = 0, l = result.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read resolver result\n      const childResult = resolveResolverResult(\n        ctx,\n        typename,\n        fieldName,\n        joinKeys(key, `${i}`),\n        select,\n        // Get the inner previous data from prevData\n        prevData != null ? prevData[i] : undefined,\n        result[i]\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childResult === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        data[i] = childResult !== undefined ? childResult : null;\n      }\n    }\n\n    return data;\n  } else if (result === null || result === undefined) {\n    return result;\n  } else if (prevData === null) {\n    // If we've previously set this piece of data to be null,\n    // we skip it and return null immediately\n    return null;\n  } else if (isDataOrKey(result)) {\n    const data = (prevData || {}) as Data;\n    return typeof result === 'string'\n      ? readSelection(ctx, result, select, data)\n      : readSelection(ctx, key, select, data, result);\n  } else {\n    warn(\n      'Invalid resolver value: The field at `' +\n        key +\n        '` is a scalar (number, boolean, etc)' +\n        ', but the GraphQL query expects a selection set for this field.',\n      9\n    );\n\n    return undefined;\n  }\n};\n\nconst resolveLink = (\n  ctx: Context,\n  link: Link | Link[],\n  typename: string,\n  fieldName: string,\n  select: SelectionSet,\n  prevData: void | null | Data | Data[]\n): DataField | undefined => {\n  if (Array.isArray(link)) {\n    const { store } = ctx;\n    const _isListNullable =\n      store.schema && isListNullable(store.schema, typename, fieldName);\n    const newLink = new Array(link.length);\n    for (let i = 0, l = link.length; i < l; i++) {\n      // Add the current index to the walked path before reading the field's value\n      ctx.__internal.path.push(i);\n      // Recursively read the link\n      const childLink = resolveLink(\n        ctx,\n        link[i],\n        typename,\n        fieldName,\n        select,\n        prevData != null ? prevData[i] : undefined\n      );\n      // After processing the field, remove the current index from the path\n      ctx.__internal.path.pop();\n      // Check the result for cache-missed values\n      if (childLink === undefined && !_isListNullable) {\n        return undefined;\n      } else {\n        newLink[i] = childLink !== undefined ? childLink : null;\n      }\n    }\n\n    return newLink;\n  } else if (link === null || prevData === null) {\n    // If the link is set to null or we previously set this piece of data to be null,\n    // we skip it and return null immediately\n    return null;\n  } else {\n    return readSelection(ctx, link, select, (prevData || {}) as Data);\n  }\n};\n\nconst isDataOrKey = (x: any): x is string | Data =>\n  typeof x === 'string' ||\n  (typeof x === 'object' && typeof (x as any).__typename === 'string');\n","import { stringifyVariables } from '@urql/core';\nimport { Variables, FieldInfo, KeyInfo } from '../types';\n\nexport const keyOfField = (fieldName: string, args?: null | Variables) =>\n  args ? `${fieldName}(${stringifyVariables(args)})` : fieldName;\n\nexport const joinKeys = (parentKey: string, key: string) =>\n  `${parentKey}.${key}`;\n\nexport const fieldInfoOfKey = (fieldKey: string): FieldInfo => {\n  const parenIndex = fieldKey.indexOf('(');\n  if (parenIndex > -1) {\n    return {\n      fieldKey,\n      fieldName: fieldKey.slice(0, parenIndex),\n      arguments: JSON.parse(fieldKey.slice(parenIndex + 1, -1)),\n    };\n  } else {\n    return {\n      fieldKey,\n      fieldName: fieldKey,\n      arguments: null,\n    };\n  }\n};\n\nexport const serializeKeys = (entityKey: string, fieldKey: string) =>\n  `${entityKey.replace(/\\./g, '%2e')}.${fieldKey}`;\n\nexport const deserializeKeyInfo = (key: string): KeyInfo => {\n  const dotIndex = key.indexOf('.');\n  const entityKey = key.slice(0, dotIndex).replace(/%2e/g, '.');\n  const fieldKey = key.slice(dotIndex + 1);\n  return { entityKey, fieldKey };\n};\n","import {\n  Operation,\n  RequestPolicy,\n  CacheOutcome,\n  makeOperation,\n} from '@urql/core';\n\n// Returns the given operation result with added cacheOutcome meta field\nexport const addCacheOutcome = (\n  operation: Operation,\n  outcome: CacheOutcome\n): Operation =>\n  makeOperation(operation.kind, operation, {\n    ...operation.context,\n    meta: {\n      ...operation.context.meta,\n      cacheOutcome: outcome,\n    },\n  });\n\n// Copy an operation and change the requestPolicy to skip the cache\nexport const toRequestPolicy = (\n  operation: Operation,\n  requestPolicy: RequestPolicy\n): Operation => {\n  return makeOperation(operation.kind, operation, {\n    ...operation.context,\n    requestPolicy,\n  });\n};\n","import {\n  Exchange,\n  formatDocument,\n  makeOperation,\n  Operation,\n  OperationResult,\n  RequestPolicy,\n  CacheOutcome,\n} from '@urql/core';\n\nimport {\n  filter,\n  combine,\n  scan,\n  map,\n  merge,\n  pipe,\n  share,\n  fromPromise,\n  fromArray,\n  take,\n  mergeMap,\n  concat,\n  empty,\n  Source,\n} from 'wonka';\n\nimport { query, write, writeOptimistic } from './operations';\nimport { makeDict, isDictEmpty } from './helpers/dict';\nimport { addCacheOutcome, toRequestPolicy } from './helpers/operation';\nimport { IntrospectionData, filterVariables, getMainOperation } from './ast';\nimport { Store, noopDataState, hydrateData, reserveLayer } from './store';\n\nimport {\n  UpdatesConfig,\n  ResolverConfig,\n  OptimisticMutationConfig,\n  KeyingConfig,\n  StorageAdapter,\n  Dependencies,\n} from './types';\n\ntype OperationResultWithMeta = OperationResult & {\n  outcome: CacheOutcome;\n  dependencies: Dependencies;\n};\n\ntype Operations = Set<number>;\ntype OperationMap = Map<number, Operation>;\ntype OptimisticDependencies = Map<number, Dependencies>;\ntype DependentOperations = Record<string, number[]>;\n\nexport interface CacheExchangeOpts {\n  updates?: Partial<UpdatesConfig>;\n  resolvers?: ResolverConfig;\n  optimistic?: OptimisticMutationConfig;\n  keys?: KeyingConfig;\n  schema?: IntrospectionData;\n  storage?: StorageAdapter;\n}\n\nexport const cacheExchange = (opts?: CacheExchangeOpts): Exchange => ({\n  forward,\n  client,\n  dispatchDebug,\n}) => {\n  const store = new Store(opts);\n\n  let hydration: void | Promise<void>;\n  if (opts && opts.storage) {\n    hydration = opts.storage.readData().then(entries => {\n      hydrateData(store.data, opts!.storage!, entries);\n    });\n  }\n\n  const optimisticKeysToDependencies: OptimisticDependencies = new Map();\n  const mutationResultBuffer: OperationResult[] = [];\n  const ops: OperationMap = new Map();\n  const blockedDependencies: Dependencies = makeDict();\n  const requestedRefetch: Operations = new Set();\n  const deps: DependentOperations = makeDict();\n\n  const isBlockedByOptimisticUpdate = (dependencies: Dependencies): boolean => {\n    for (const dep in dependencies) if (blockedDependencies[dep]) return true;\n    return false;\n  };\n\n  const collectPendingOperations = (\n    pendingOperations: Operations,\n    dependencies: void | Dependencies\n  ) => {\n    if (dependencies) {\n      // Collect operations that will be updated due to cache changes\n      for (const dep in dependencies) {\n        const keys = deps[dep];\n        if (keys) {\n          deps[dep] = [];\n          for (let i = 0, l = keys.length; i < l; i++) {\n            pendingOperations.add(keys[i]);\n          }\n        }\n      }\n    }\n  };\n\n  const executePendingOperations = (\n    operation: Operation,\n    pendingOperations: Operations\n  ) => {\n    // Reexecute collected operations and delete them from the mapping\n    pendingOperations.forEach(key => {\n      if (key !== operation.key) {\n        const op = ops.get(key);\n        if (op) {\n          ops.delete(key);\n          let policy: RequestPolicy = 'cache-first';\n          if (requestedRefetch.has(key)) {\n            requestedRefetch.delete(key);\n            policy = 'cache-and-network';\n          }\n          client.reexecuteOperation(toRequestPolicy(op, policy));\n        }\n      }\n    });\n  };\n\n  // This registers queries with the data layer to ensure commutativity\n  const prepareForwardedOperation = (operation: Operation) => {\n    if (operation.kind === 'query') {\n      // Pre-reserve the position of the result layer\n      reserveLayer(store.data, operation.key);\n    } else if (operation.kind === 'teardown') {\n      // Delete reference to operation if any exists to release it\n      ops.delete(operation.key);\n      // Mark operation layer as done\n      noopDataState(store.data, operation.key);\n    } else if (\n      operation.kind === 'mutation' &&\n      operation.context.requestPolicy !== 'network-only'\n    ) {\n      // This executes an optimistic update for mutations and registers it if necessary\n      const { dependencies } = writeOptimistic(store, operation, operation.key);\n      if (!isDictEmpty(dependencies)) {\n        // Update blocked optimistic dependencies\n        for (const dep in dependencies) {\n          blockedDependencies[dep] = true;\n        }\n\n        // Store optimistic dependencies for update\n        optimisticKeysToDependencies.set(operation.key, dependencies);\n\n        // Update related queries\n        const pendingOperations: Operations = new Set();\n        collectPendingOperations(pendingOperations, dependencies);\n        executePendingOperations(operation, pendingOperations);\n      }\n    }\n\n    return makeOperation(\n      operation.kind,\n      {\n        key: operation.key,\n        query: formatDocument(operation.query),\n        variables: operation.variables\n          ? filterVariables(\n              getMainOperation(operation.query),\n              operation.variables\n            )\n          : operation.variables,\n      },\n      operation.context\n    );\n  };\n\n  // This updates the known dependencies for the passed operation\n  const updateDependencies = (op: Operation, dependencies: Dependencies) => {\n    for (const dep in dependencies) {\n      (deps[dep] || (deps[dep] = [])).push(op.key);\n      ops.set(op.key, op);\n    }\n  };\n\n  // Retrieves a query result from cache and adds an `isComplete` hint\n  // This hint indicates whether the result is \"complete\" or not\n  const operationResultFromCache = (\n    operation: Operation\n  ): OperationResultWithMeta => {\n    const res = query(store, operation);\n    const cacheOutcome: CacheOutcome = res.data\n      ? !res.partial\n        ? 'hit'\n        : 'partial'\n      : 'miss';\n\n    updateDependencies(operation, res.dependencies);\n\n    return {\n      outcome: cacheOutcome,\n      operation,\n      data: res.data,\n      dependencies: res.dependencies,\n    };\n  };\n\n  // Take any OperationResult and update the cache with it\n  const updateCacheWithResult = (\n    result: OperationResult,\n    pendingOperations: Operations\n  ): OperationResult => {\n    const { operation, error, extensions } = result;\n    const { key } = operation;\n\n    if (operation.kind === 'mutation') {\n      // Collect previous dependencies that have been written for optimistic updates\n      const dependencies = optimisticKeysToDependencies.get(key);\n      collectPendingOperations(pendingOperations, dependencies);\n      optimisticKeysToDependencies.delete(key);\n    } else {\n      reserveLayer(store.data, operation.key);\n    }\n\n    let queryDependencies: void | Dependencies;\n    if (result.data) {\n      // Write the result to cache and collect all dependencies that need to be\n      // updated\n      const writeDependencies = write(\n        store,\n        operation,\n        result.data,\n        result.error,\n        key\n      ).dependencies;\n      collectPendingOperations(pendingOperations, writeDependencies);\n\n      const queryResult = query(\n        store,\n        operation,\n        result.data,\n        result.error,\n        key\n      );\n      result.data = queryResult.data;\n      if (operation.kind === 'query') {\n        // Collect the query's dependencies for future pending operation updates\n        queryDependencies = queryResult.dependencies;\n        collectPendingOperations(pendingOperations, queryDependencies);\n      }\n    } else {\n      noopDataState(store.data, operation.key);\n    }\n\n    // Update this operation's dependencies if it's a query\n    if (queryDependencies) {\n      updateDependencies(result.operation, queryDependencies);\n    }\n\n    return { data: result.data, error, extensions, operation };\n  };\n\n  return ops$ => {\n    const sharedOps$ = pipe(ops$, share);\n\n    // Buffer operations while waiting on hydration to finish\n    // If no hydration takes place we replace this stream with an empty one\n    const bufferedOps$ = hydration\n      ? pipe(\n          combine(\n            pipe(\n              sharedOps$,\n              scan((acc: Operation[], x) => (acc.push(x), acc), [])\n            ),\n            fromPromise(hydration)\n          ),\n          take(1),\n          mergeMap(zip => fromArray(zip[0]))\n        )\n      : (empty as Source<Operation>);\n\n    const inputOps$ = pipe(concat([bufferedOps$, sharedOps$]), share);\n\n    // Filter by operations that are cacheable and attempt to query them from the cache\n    const cacheOps$ = pipe(\n      inputOps$,\n      filter(op => {\n        return (\n          op.kind === 'query' && op.context.requestPolicy !== 'network-only'\n        );\n      }),\n      map(operationResultFromCache),\n      share\n    );\n\n    const nonCacheOps$ = pipe(\n      inputOps$,\n      filter(op => {\n        return (\n          op.kind !== 'query' || op.context.requestPolicy === 'network-only'\n        );\n      })\n    );\n\n    // Rebound operations that are incomplete, i.e. couldn't be queried just from the cache\n    const cacheMissOps$ = pipe(\n      cacheOps$,\n      filter(res => {\n        return (\n          res.outcome === 'miss' &&\n          res.operation.context.requestPolicy !== 'cache-only' &&\n          !isBlockedByOptimisticUpdate(res.dependencies)\n        );\n      }),\n      map(res => {\n        dispatchDebug({\n          type: 'cacheMiss',\n          message: 'The result could not be retrieved from the cache',\n          operation: res.operation,\n        });\n        return addCacheOutcome(res.operation, 'miss');\n      })\n    );\n\n    // Resolve OperationResults that the cache was able to assemble completely and trigger\n    // a network request if the current operation's policy is cache-and-network\n    const cacheResult$ = pipe(\n      cacheOps$,\n      filter(\n        res =>\n          res.outcome !== 'miss' ||\n          res.operation.context.requestPolicy === 'cache-only'\n      ),\n      map(\n        (res: OperationResultWithMeta): OperationResult => {\n          const { operation, outcome, dependencies } = res;\n          const result: OperationResult = {\n            operation: addCacheOutcome(operation, outcome),\n            data: res.data,\n            error: res.error,\n            extensions: res.extensions,\n          };\n\n          if (\n            operation.context.requestPolicy === 'cache-and-network' ||\n            (operation.context.requestPolicy === 'cache-first' &&\n              outcome === 'partial')\n          ) {\n            result.stale = true;\n            if (!isBlockedByOptimisticUpdate(dependencies)) {\n              client.reexecuteOperation(\n                toRequestPolicy(operation, 'network-only')\n              );\n            } else if (\n              operation.context.requestPolicy === 'cache-and-network'\n            ) {\n              requestedRefetch.add(operation.key);\n            }\n          }\n\n          dispatchDebug({\n            type: 'cacheHit',\n            message: `A requested operation was found and returned from the cache.`,\n            operation: res.operation,\n            data: {\n              value: result,\n            },\n          });\n\n          return result;\n        }\n      )\n    );\n\n    // Forward operations that aren't cacheable and rebound operations\n    // Also update the cache with any network results\n    const result$ = pipe(\n      merge([nonCacheOps$, cacheMissOps$]),\n      map(prepareForwardedOperation),\n      forward,\n      share\n    );\n\n    // Results that can immediately be resolved\n    const nonOptimisticResults$ = pipe(\n      result$,\n      filter(result => !optimisticKeysToDependencies.has(result.operation.key)),\n      map(result => {\n        const pendingOperations: Operations = new Set();\n        // Update the cache with the incoming API result\n        const cacheResult = updateCacheWithResult(result, pendingOperations);\n        // Execute all dependent queries\n        executePendingOperations(result.operation, pendingOperations);\n        return cacheResult;\n      })\n    );\n\n    // Prevent mutations that were previously optimistic from being flushed\n    // immediately and instead clear them out slowly\n    const optimisticMutationCompletion$ = pipe(\n      result$,\n      filter(result => optimisticKeysToDependencies.has(result.operation.key)),\n      mergeMap(\n        (result: OperationResult): Source<OperationResult> => {\n          const length = mutationResultBuffer.push(result);\n          if (length < optimisticKeysToDependencies.size) {\n            return empty;\n          }\n\n          for (let i = 0; i < mutationResultBuffer.length; i++) {\n            reserveLayer(store.data, mutationResultBuffer[i].operation.key);\n          }\n\n          for (const dep in blockedDependencies) {\n            delete blockedDependencies[dep];\n          }\n\n          const results: OperationResult[] = [];\n          const pendingOperations: Operations = new Set();\n\n          let bufferedResult: OperationResult | void;\n          while ((bufferedResult = mutationResultBuffer.shift()))\n            results.push(\n              updateCacheWithResult(bufferedResult, pendingOperations)\n            );\n\n          // Execute all dependent queries as a single batch\n          executePendingOperations(result.operation, pendingOperations);\n\n          return fromArray(results);\n        }\n      )\n    );\n\n    return merge([\n      nonOptimisticResults$,\n      optimisticMutationCompletion$,\n      cacheResult$,\n    ]);\n  };\n};\n","import { pipe, merge, makeSubject, share, filter } from 'wonka';\nimport { print, SelectionNode } from 'graphql';\n\nimport {\n  Operation,\n  Exchange,\n  ExchangeIO,\n  CombinedError,\n  createRequest,\n  makeOperation,\n} from '@urql/core';\n\nimport {\n  getMainOperation,\n  getFragments,\n  isInlineFragment,\n  isFieldNode,\n  shouldInclude,\n  getSelectionSet,\n  getName,\n} from './ast';\n\nimport {\n  SerializedRequest,\n  OptimisticMutationConfig,\n  Variables,\n} from './types';\n\nimport { makeDict } from './helpers/dict';\nimport { cacheExchange, CacheExchangeOpts } from './cacheExchange';\nimport { toRequestPolicy } from './helpers/operation';\n\n/** Determines whether a given query contains an optimistic mutation field */\nconst isOptimisticMutation = (\n  config: OptimisticMutationConfig,\n  operation: Operation\n) => {\n  const vars: Variables = operation.variables || makeDict();\n  const fragments = getFragments(operation.query);\n  const selections = [...getSelectionSet(getMainOperation(operation.query))];\n\n  let field: void | SelectionNode;\n  while ((field = selections.pop())) {\n    if (!shouldInclude(field, vars)) {\n      continue;\n    } else if (!isFieldNode(field)) {\n      const fragmentNode = !isInlineFragment(field)\n        ? fragments[getName(field)]\n        : field;\n      if (fragmentNode) selections.push(...getSelectionSet(fragmentNode));\n    } else if (config[getName(field)]) {\n      return true;\n    }\n  }\n\n  return false;\n};\n\nconst isOfflineError = (error: undefined | CombinedError) =>\n  error &&\n  error.networkError &&\n  !error.response &&\n  ((typeof navigator !== 'undefined' && navigator.onLine === false) ||\n    /request failed|failed to fetch|network\\s?error/i.test(\n      error.networkError.message\n    ));\n\nexport const offlineExchange = (opts: CacheExchangeOpts): Exchange => input => {\n  const { storage } = opts;\n\n  if (\n    storage &&\n    storage.onOnline &&\n    storage.readMetadata &&\n    storage.writeMetadata\n  ) {\n    const { forward: outerForward, client, dispatchDebug } = input;\n    const { source: reboundOps$, next } = makeSubject<Operation>();\n    const optimisticMutations = opts.optimistic || {};\n    const failedQueue: Operation[] = [];\n\n    const updateMetadata = () => {\n      const requests: SerializedRequest[] = [];\n      for (let i = 0; i < failedQueue.length; i++) {\n        const operation = failedQueue[i];\n        if (operation.kind === 'mutation') {\n          requests.push({\n            query: print(operation.query),\n            variables: operation.variables,\n          });\n        }\n      }\n      storage.writeMetadata!(requests);\n    };\n\n    let isFlushingQueue = false;\n    const flushQueue = () => {\n      if (!isFlushingQueue) {\n        isFlushingQueue = true;\n\n        for (let i = 0; i < failedQueue.length; i++) {\n          const operation = failedQueue[i];\n          if (operation.kind === 'mutation') {\n            next(makeOperation('teardown', operation));\n          }\n        }\n\n        for (let i = 0; i < failedQueue.length; i++)\n          client.reexecuteOperation(failedQueue[i]);\n\n        failedQueue.length = 0;\n        isFlushingQueue = false;\n        updateMetadata();\n      }\n    };\n\n    const forward: ExchangeIO = ops$ => {\n      return pipe(\n        outerForward(ops$),\n        filter(res => {\n          if (\n            res.operation.kind === 'mutation' &&\n            isOfflineError(res.error) &&\n            isOptimisticMutation(optimisticMutations, res.operation)\n          ) {\n            failedQueue.push(res.operation);\n            updateMetadata();\n            return false;\n          }\n\n          return true;\n        })\n      );\n    };\n\n    storage.onOnline(flushQueue);\n    storage.readMetadata().then(mutations => {\n      if (mutations) {\n        for (let i = 0; i < mutations.length; i++) {\n          failedQueue.push(\n            client.createRequestOperation(\n              'mutation',\n              createRequest(mutations[i].query, mutations[i].variables)\n            )\n          );\n        }\n\n        flushQueue();\n      }\n    });\n\n    const cacheResults$ = cacheExchange(opts)({\n      client,\n      dispatchDebug,\n      forward,\n    });\n\n    return ops$ => {\n      const sharedOps$ = share(ops$);\n      const opsAndRebound$ = merge([reboundOps$, sharedOps$]);\n\n      return pipe(\n        cacheResults$(opsAndRebound$),\n        filter(res => {\n          if (res.operation.kind === 'query' && isOfflineError(res.error)) {\n            next(toRequestPolicy(res.operation, 'cache-only'));\n            failedQueue.push(res.operation);\n            return false;\n          }\n\n          return true;\n        })\n      );\n    };\n  }\n\n  return cacheExchange(opts)(input);\n};\n"]},"metadata":{},"sourceType":"module"}